{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62eacf4b-7dfc-4e88-84c1-b7b7289134b1",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA) Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f3bf04-01d6-4a39-b53e-682ccba7e86e",
   "metadata": {},
   "source": [
    "This notebook contains an applied example of using LDA for topic modelling, roughly following the example [here](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0).    \n",
    "Please note that this notebook shows how to run LDA, but does not explain how it works. For more information on how LDA works, see [this article](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b76339-4454-4c4d-8c89-d9ade979ce27",
   "metadata": {},
   "source": [
    "---\n",
    "**Load packages:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f119ca4-90bd-402b-9020-67622baac7d2",
   "metadata": {},
   "source": [
    "Note: If you've never run this before, you might need to run `python3 -m spacy download en_core_web_sm` in the terminal first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a8c99-8312-464f-b15c-9f967faf7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import spacy # spaCy for preprocessin\n",
    "\n",
    "# Set random seed\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b816fcd-b2e5-4dc8-9822-01398fb586c4",
   "metadata": {},
   "source": [
    "---\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date. More information can be found here: https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "\n",
    "**Fetch data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11fdc2-1e5f-43bb-9e73-ab2609097c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))\n",
    "categories = newsgroups.target_names\n",
    "target_num = newsgroups.target\n",
    "target = [categories[x] for x in target_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1b3d4-81b0-4bc7-bd3a-ff1d8ea19ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['text'] = newsgroups.data\n",
    "df['target'] = target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d56770-793f-4733-af23-78969a62f55c",
   "metadata": {},
   "source": [
    "----\n",
    "**Clean data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c9f70-526a-400c-86be-b5ce4e2dd115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove newlines\n",
    "df['text_processed'] = df['text'].map(lambda x: re.sub('[\\n\\t]', ' ', x))\n",
    "# Remove punctuation\n",
    "df['text_processed'] = df['text_processed'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "# Lowercase\n",
    "df['text_processed'] = df['text_processed'].map(lambda x: x.lower())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787f95c-c23b-4120-8a51-8cec4542ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve orig df\n",
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f1bfb-e0c1-4bc1-9e27-27defaf47a8a",
   "metadata": {},
   "source": [
    "Filter the dataset down into just 1 subcategory per category for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80350164-cef2-4501-9d31-60f89ee7e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df.target.str.split('.').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ea634-212b-42c0-993c-94cdd8cf8da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_targets = df.loc[:, [\"category\", \"target\"]].drop_duplicates()\n",
    "chosen_targets = []\n",
    "for i in unq_targets.category.unique():\n",
    "    choice = unq_targets[unq_targets.category == i].iloc[[0]].target.iloc[0]\n",
    "    chosen_targets.append(choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ef4d1-6135-419e-8e0f-0de19f6b56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df.target.isin(chosen_targets),].reset_index(drop = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb556854-6f61-4cbd-9025-131f38481624",
   "metadata": {},
   "source": [
    "-----\n",
    "**Wordcloud:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcdc200-c60a-4fe0-8f8d-8aabaae007e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(df['text_processed'].values))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d33a18-f48d-46b1-b859-d71799e24ee0",
   "metadata": {},
   "source": [
    "------\n",
    "**Prepare data for LDA:**\n",
    "\n",
    "- Tokenize text\n",
    "- Remove stopwords\n",
    "- Convery to corpus and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab59b8-0a31-468d-a424-5449c1c1a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4759fc1d-109e-45cc-ad86-2ad1fcb7db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  #deacc=True removes punctuations\n",
    "\n",
    "data = df.text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d882f3b-ac46-49ca-9a26-f2c2209ba133",
   "metadata": {},
   "source": [
    "**Bigram and trigram models**  \n",
    "https://medium.com/analytics-vidhya/topic-modeling-using-gensim-lda-in-python-48eaa2344920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae48bbc-8a25-45f5-bb8c-52c4e4f75b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cc9ca7-7feb-4b6a-98b9-b6f726e5ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4c0b67-2494-4c22-8c82-60421eb51a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "#python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c83d0-ae6d-4124-bcb4-48c81d7a167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb408e6-106d-4c38-a0c1-a854f3d4830b",
   "metadata": {},
   "source": [
    "We can see how the text is reduced to a bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980d4f1-c73b-465b-84ed-64d70001d1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(texts[10])\n",
    "print(df.text_processed.iloc[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd4284-44c2-462f-a0f2-49ec1de82814",
   "metadata": {},
   "source": [
    "---------\n",
    "**Model training:**\n",
    "\n",
    "To keep things simple, we’ll keep all the parameters to default except for inputting the number of topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9836b-90e9-4de2-8867-3f036b4980de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 7 # This is our expected number of categories\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213fdfff-7811-414d-9b74-b98773370779",
   "metadata": {},
   "source": [
    "These topics don't look very good - from the words they all look pretty similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb81be3-adc6-4903-bbc8-d46d70a46bcc",
   "metadata": {},
   "source": [
    "-------\n",
    "**Analyzing LDA model results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319350e-9eb8-4dab-8671-3a288a49ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pyLDAvis\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8a1f1-1e63-42c9-93c5-46fb9a5ae100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577bb409-4c15-4aee-8033-b532c644391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c35e8-5f6e-4391-8f0c-470c91733aad",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537fd48-431c-4747-89af-95dc7645c7e8",
   "metadata": {},
   "source": [
    "**Evaluating topic predictions**  \n",
    "Generate list of assigned topics and probabilites for each text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbd735-2a02-4b88-ad1a-8be6f2a26ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_result = []\n",
    "topic_prob = []\n",
    "for i in corpus:\n",
    "    topic_result.append(lda_model[i][0][0])\n",
    "    topic_prob.append(lda_model[i][0][1])\n",
    "    \n",
    "df[\"topic_result\"] = topic_result\n",
    "df[\"topic_prob\"] = topic_prob\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5dafe-cbb8-4366-a9a4-06bdd2ca65f0",
   "metadata": {},
   "source": [
    "We can use a confusion matrix to see how our text is classified by the model vs. the actual topic:\n",
    "\n",
    "**Confusion matrix:** (https://towardsdatascience.com/5-ways-to-use-a-seaborn-heatmap-python-tutorial-c79950f5add3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d6672-4ba1-42d7-b26e-3f12ac95dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbd2c8-c869-43c7-9dad-539cfc98e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = pd.crosstab(df['target'], df['topic_result'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1305eac-50ec-4f32-b86c-9f6519e4b8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "sns.heatmap(conf_matrix,\n",
    "            cmap='coolwarm',\n",
    "            annot=True,\n",
    "            fmt='.5g',\n",
    "            vmax=200)\n",
    "\n",
    "plt.xlabel('Predicted',fontsize=22)\n",
    "plt.ylabel('Actual',fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022ac54-a02f-4de7-8c7b-756f387474fa",
   "metadata": {},
   "source": [
    "It doesn't look like it does too well in distinguishing the subject matter - lots of entries are classified in '0'. What if we get rid of this - does the pattern become clearer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36402c2a-b548-495c-85cb-847d6dd2559e",
   "metadata": {},
   "source": [
    "Look at specific topics - 3 and 5 - as they look like they have stronger relationships with certain categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3f05b-72e4-4792-a410-71eeff00215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_no = 3\n",
    "print(lda_model.print_topics()[topic_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51988e35-f006-425d-a7d7-91eec1b925c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.topic_result == topic_no].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667bd01a-cfde-4de7-b905-8f1e0dd95993",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"---- Data row\", i, \"--------------------------\")\n",
    "    print(\"Actual category:\", df.loc[df.topic_result == topic_no]['target'].iloc[i])\n",
    "    print(df.loc[df.topic_result == topic_no]['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e24b30c-c7ba-4995-9d38-15d9a57dd2a4",
   "metadata": {},
   "source": [
    "----- \n",
    "**Filtering to topics with probability threshold**  \n",
    "What if we only selected rows with certain topic_prob?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee078ff4-888a-4cd8-92a2-896b4eb1ac9d",
   "metadata": {},
   "source": [
    "Look at distribution of topic probabilities for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612fc28-55d0-4b16-b575-fb42ff6e6dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the values in descending order\n",
    "sorted_values = df['topic_prob'].sort_values(ascending = False)\n",
    "\n",
    "# Calculate cumulative frequencies\n",
    "cumulative_frequencies = sorted_values.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7a4dd-a308-48ad-a8c2-893537fac106",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a980e5-7eab-486c-a6de-152b86ed70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cumulative frequency plot\n",
    "plt.plot(sorted_values, cumulative_frequencies, marker='o')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Cumulative Frequency')\n",
    "plt.title('Cumulative Frequency Plot')\n",
    "plt.grid(True)\n",
    "plt.xlim(max(sorted_values), min(sorted_values))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec4c7d-b245-40a8-b993-07e3e8365a65",
   "metadata": {},
   "source": [
    "Filter dataset to look at just texts where probability exceeds thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80aa847-a71a-4b2f-b412-14a9f92c964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set probability threshold\n",
    "prob_thresh = 0.9\n",
    "df_prob_thresh = df.loc[df.topic_prob >= prob_thresh]\n",
    "df_prop_over_thresh = round((df_prob_thresh.shape[0]/df.shape[0])*100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c41bd3-4c3e-421c-9ad2-d4f6c5c7ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\".join([str(df_prop_over_thresh), \"%\"]), \"of texts have topic probability threshold of\", prob_thresh, \"or above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c84750-decc-41e5-8d5d-7369d838b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = pd.crosstab(df_prob_thresh['target'], df_prob_thresh['topic_result'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1febd3a-7da1-434d-9bca-a4861f7fa898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "sns.heatmap(conf_matrix,\n",
    "            cmap='coolwarm',\n",
    "            annot=True,\n",
    "            fmt='.5g',\n",
    "            vmax=200)\n",
    "\n",
    "plt.xlabel('Predicted',fontsize=22)\n",
    "plt.ylabel('Actual',fontsize=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52bb8cd-7bb5-4ef2-9e7a-b3be809c7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_no = 4\n",
    "print(lda_model.print_topics()[topic_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6fa4c-e94e-4bf1-9b33-272868113fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob_thresh.loc[df_prob_thresh.topic_result == topic_no].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1d8c2-eaa8-41e3-99be-0bbbb02cf045",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82fb4a-624a-48de-ac50-c7857dd50293",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "How can we judge how well our topic model works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59092055-319d-45dd-ae8b-c846ee6afc2a",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#13viewthetopicsinldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc34f59-5c8c-4366-a830-19827e590209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e366b4ab-24a4-44e3-a449-ef15313b5ea1",
   "metadata": {},
   "source": [
    "- Lower the perplexity better the model.   \n",
    "- Higher the topic coherence, the topic is more human interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76d18b-247c-4059-997d-f23d212f9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd698a-1071-4da8-89dd-48453a4240dd",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "## Improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33334acf-b8d9-4980-a1f9-bacccfa55148",
   "metadata": {},
   "source": [
    "It could be that LDA doesn't work particularly well on this data because of the short text length.\n",
    "\n",
    "\n",
    "**ChatGPT: \n",
    "Does LDA work well for short texts?**\n",
    "\n",
    "_LDA is generally more effective when applied to longer texts compared to short texts. This is because short texts often lack sufficient context and word co-occurrence patterns to accurately determine topic assignments. However, there are variations and adaptations of LDA that have been developed specifically for short texts._\n",
    "\n",
    "_When dealing with short texts, such as tweets or product reviews, the limited amount of text can lead to sparsity and ambiguity in the word distributions. This can make it challenging for LDA to reliably identify meaningful topics. Additionally, short texts often lack the necessary depth and breadth of information that longer documents provide._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd49806-e254-4399-9090-f055c7e5e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore number of words in texts\n",
    "df[\"words\"] = df[\"text\"].apply(lambda n: len(n.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a1fc7-1f82-4202-842c-71f152e08624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the histogram\n",
    "plt.hist(df['words'], bins='auto')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Setting the x-axis limits\n",
    "x_min = 0  # Replace with the desired minimum x-axis value\n",
    "x_max = 500  # Replace with the desired maximum x-axis value\n",
    "plt.xlim(x_min, x_max)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f856659-6b81-464b-88a8-d0cc5b70554e",
   "metadata": {},
   "source": [
    "LDA typically doesn't work so well for <= 50 words. What proprtion of our data is this true for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59171f-7a20-4348-99e5-39ac2250ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.words <= 50].shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77954f9a-d681-4160-88f2-a5da9daa6b4e",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "## New model\n",
    "Note: this model doesn't include use of bigrams and lemmatization. Any improved results could be down to removing this step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842901a0-35f3-402d-96d2-c7da4f9ed96e",
   "metadata": {},
   "source": [
    "Would be interesting to run the model on just the longer texts.   \n",
    "Obviously this wouldn't work in practice when applying the technique, but is interesting here to see whether LDA becomes more effective with longer texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4033830-eb3e-4a59-92ff-d2575bdd6503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove shorter texts\n",
    "df_long = df.loc[df.words > 50]\n",
    "df_long.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf83bd-db67-4b3b-9e53-541b1805a3fe",
   "metadata": {},
   "source": [
    "Run basic model (note, this doesn't include the use of bigrams):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae304ab-5279-4467-b8a1-4a6b1f6b35b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_long = df_long.text_processed.values.tolist()\n",
    "data_words_long = list(sent_to_words(data_long))\n",
    "# remove stop words\n",
    "data_words_long = remove_stopwords(data_words_long)\n",
    "print(data_words_long[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdfa563-8c82-4a3a-a796-837acf0dc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word_long = corpora.Dictionary(data_words_long)\n",
    "\n",
    "# Create Corpus\n",
    "texts_long = data_words_long\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus_long = [id2word_long.doc2bow(text) for text in texts_long]\n",
    "\n",
    "# View\n",
    "print(corpus_long[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e441f-04eb-4e76-b972-e52caf014540",
   "metadata": {},
   "source": [
    "We can see how the text is reduced to a bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f3ad4-a0a7-4d28-8937-c97111e8fe4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(texts_long[10])\n",
    "print(df_long.text_processed.iloc[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa961f-1ce0-4bc9-be35-84b3c73a9199",
   "metadata": {},
   "source": [
    "---------\n",
    "**Model training:**\n",
    "\n",
    "To keep things simple, we’ll keep all the parameters to default except for inputting the number of topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b25ac6-03d5-47c6-8cd4-681a481b329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 7 # This is our expected number of categories\n",
    "\n",
    "# Build LDA model\n",
    "lda_model_long = gensim.models.LdaMulticore(corpus=corpus_long,\n",
    "                                       id2word=id2word_long,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model_long.print_topics())\n",
    "doc_lda_long = lda_model_long[corpus_long]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac21edd-62ed-4597-8da0-a1cffae077fb",
   "metadata": {},
   "source": [
    "These topics don't look very good - from the words they all look pretty similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a8a63-655e-4016-aca5-f9e905fbcf85",
   "metadata": {},
   "source": [
    "-------\n",
    "**Analyzing LDA model results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6101c8-6070-4923-a573-167981608a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pyLDAvis\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf02f5e-ed2d-461b-aad9-fd2b897b693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared_long = pyLDAvis.gensim.prepare(lda_model_long, corpus_long, id2word_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a502c5c7-5e82-4224-a7c6-68f6e0e19b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6c545-1c27-44bc-86bf-3f474ad6fc40",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb864b-f376-4d2e-9275-02eb3f7fad7b",
   "metadata": {},
   "source": [
    "**Evaluating topic predictions**  \n",
    "Generate list of assigned topics and probabilites for each text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db068c-7e56-40d9-979e-33be17ba75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_result = []\n",
    "topic_prob = []\n",
    "for i in corpus_long:\n",
    "    topic_result.append(lda_model_long[i][0][0])\n",
    "    topic_prob.append(lda_model_long[i][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a0539-f449-4224-bcdd-65368fe5745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long[\"topic_result\"] = topic_result\n",
    "df_long[\"topic_prob\"] = topic_prob\n",
    "df_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7c20a-4e75-4bc3-b986-fda915b5db4c",
   "metadata": {},
   "source": [
    "We can use a confusion matrix to see how our text is classified by the model vs. the actual topic:\n",
    "\n",
    "**Confusion matrix:** (https://towardsdatascience.com/5-ways-to-use-a-seaborn-heatmap-python-tutorial-c79950f5add3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ccbf3-1e90-41cf-93da-26ba8fcb6ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7706193-7da2-4481-ba43-2db4615d2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_long = pd.crosstab(df_long['target'], df_long['topic_result'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (conf_matrix_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6060b5c7-1bce-4f8c-a41f-bc7aefd77c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "sns.heatmap(conf_matrix_long,\n",
    "            cmap='coolwarm',\n",
    "            annot=True,\n",
    "            fmt='.5g',\n",
    "            vmax=200)\n",
    "\n",
    "plt.xlabel('Predicted',fontsize=22)\n",
    "plt.ylabel('Actual',fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea32f1d-8e8c-442b-a77a-188064517861",
   "metadata": {},
   "source": [
    "This looks like it might be working slighly better - topic 2 seems to be religion and politics, which could have similar language, while 5 is computing and science which are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb2e3a-954d-4f7f-887c-8fdf8e63a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_no = 2\n",
    "print(lda_model.print_topics()[topic_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9dd18d-72b7-4032-8844-ea0c5ab26dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.loc[df_long.topic_result == topic_no].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84780e4-8075-4a92-89c1-2d4243d37272",
   "metadata": {},
   "source": [
    "- Lower the perplexity better the model.   \n",
    "- Higher the topic coherence, the topic is more human interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4705c-c0fb-418e-a831-e54a0b50cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model_long.log_perplexity(corpus_long))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda_long = CoherenceModel(model=lda_model_long, texts=data_words_long, dictionary=id2word_long, coherence='c_v')\n",
    "coherence_lda_long = coherence_model_lda_long.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532b4c7-5c56-4fca-b4ec-92a8e2f99404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-code-examples-venv",
   "language": "python",
   "name": "nlp-code-examples-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
