{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07687c6-f6e7-4797-8a93-e4bb09b6b628",
   "metadata": {},
   "source": [
    "# Text classification introduction & example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39069705-9402-4b0c-b497-389002ad51eb",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a080e-9a3c-4bb3-90a4-bdab8c6eb088",
   "metadata": {},
   "source": [
    "1. [What is text classification?](#what_is_text_classification)\n",
    "    1. [Instances in which text classification is used](#text_classification_instances)\n",
    "    2. [Note on supervised learning](#supervised_learning)\n",
    "\n",
    "\n",
    "2. [Pros and cons of text classification](#pros_and_cons)\n",
    "\n",
    "\n",
    "3. [Text classification steps](#steps)\n",
    "\n",
    "\n",
    "4. [Text classification example - spam text message](#example_start)\n",
    "    1. [Text pre-processing](#text_pre_processing)\n",
    "    2. [Feature extraction](#feature_extraction)\n",
    "    3. [Model training](#model_training)\n",
    "    4. [Model Evaluation](#model_evaluation)\n",
    "\n",
    "\n",
    "5. [Some further notes](#further_notes)\n",
    "    1. [Balanced datasets](#balanced_datasets)\n",
    "    2. [Exploratory data analysis](#EDA)\n",
    "\n",
    "\n",
    "6. [Further resources](#resources) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d96493-c803-4dbf-87c8-612e63bb7134",
   "metadata": {},
   "source": [
    "# 1. What is text classification? <a name=\"what_is_text_classification\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a5877-0a68-4bc5-9112-f693e256c2bb",
   "metadata": {},
   "source": [
    "Text classification is a common Natural Language Processing (NLP) task. \n",
    "\n",
    "It involves assigning labels or categories to text documents based on their contents. \n",
    "\n",
    "The goal is to automatically analyse and organise text into pre-defined categories or classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d8e02-24ad-4c7e-a3d1-d0a3a908a668",
   "metadata": {},
   "source": [
    "## 1.1. Instances in which text classification is used <a name=\"text_classification_instances\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea97555-e42a-4087-9aa5-db7254f1acd2",
   "metadata": {},
   "source": [
    "1. Spam detection: used to identify and filter out spam emails from legitimate ones. \n",
    "\n",
    "2. Sentiment analysis: used to analyse text and understand the underlying sentiment, usually positive or negative. \n",
    "\n",
    "3. Document classification: search engines use text classification to categorise and index web pages.\n",
    "\n",
    "There are many more applications, these are just a few examples.\n",
    "\n",
    "### An MOJ example\n",
    "\n",
    "In the MOJ, the data science hub has done text classification projects, such as the Invoice Tagging project, which is still extensively used by finance. \n",
    "\n",
    "Invoices (>Â£25k) are given a plain English description before publication. The Plain English Description is essentially a set of categories that each invoice fits into.\n",
    "\n",
    "Before this project, a human had to read each invoice description and assign a category by hand, which was time consuming. \n",
    "\n",
    "By using machine learning, a model was created to understand the relevant invoice information well enough to accurately assign the correct Plain English Description to each invoice, saving days of work for finance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b4c12-966e-44f6-ba80-02916cf9ff3a",
   "metadata": {},
   "source": [
    "## 1.2. Note on supervised learning <a name=\"supervised_learning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74e076-fae0-4908-af44-71894590bc06",
   "metadata": {},
   "source": [
    "Text classification is a supervised machine learning task. But what does this actually mean? \n",
    "\n",
    "In this context, it means that you have some group of texts and you know each one's label/category. \n",
    "The dataset usually contains input variables, known as features, as well as the output variable, known as the target. \n",
    "\n",
    "Essentially, the dataset that you have already has the user characteristics and the answer to the question you're trying to answer.\n",
    "\n",
    "For example, let's say that you want to build a model that correctly classifies whether an article is about politics, sports, education etc. In a supervised learning context, the data that you have would contain the text of the article as well as a label that tells you what kind of article it is. \n",
    "\n",
    "This then allows us to fit a model to the data, using the answers to help us refine our results iteratively.\n",
    "\n",
    "The machine learning algorithm will try to learn the relationship between features and target and will usually apply these \"learnings\" to unlabeled data to try and classify it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e2e39-1931-4e20-bfb6-010f067a6e55",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Pros and cons of text classification <a name=\"pros_and_cons\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c257d10-1443-45ae-8504-76541337a612",
   "metadata": {},
   "source": [
    "## 2.1. Pros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b126683-7705-45f0-8227-01fe37768c7a",
   "metadata": {},
   "source": [
    "1. Scalability: can handle massive datasets, making these models suitable for dealing with large amounts of data. \n",
    "\n",
    "2. Automation: automates the process of organising large volumes of unstructured text data into pre-determined categories, saving time and effort. \n",
    "\n",
    "3. Consistency and accuracy: given a model is properly trained, it will correctly categorise text, reducing the risk of bias and human error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c74180-bfa9-4c3c-b14d-7644852cc4e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2. Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d4192-3570-4520-a140-9ff2a88676a8",
   "metadata": {},
   "source": [
    "1. Ambiguity and noise: models may struggle with ambiguous or noisy data, such as misspellings, abbreviations, slang or sarcasm which can lead to inaccuracies.\n",
    "\n",
    "2. Domain specificity: models trained on one domain may not generalise well to other domains due to differences in vocabolary, writing styles or context, requiring domain-specific training data and fine-tuning.\n",
    "\n",
    "3. Imbalance and bias: class imbalance in the training data where certain classes are under-represented can lead to biased models with inaccurate predictions. \n",
    "\n",
    "4. Feature engineering complexity and interpretability: extracting useful information from text data may require parameter tuning to improve model performance and interpreting results may be challenging. \n",
    "\n",
    "Remember that your model will be as good as the data you feed it to learn. \n",
    "\n",
    "If you're training the model for a specific task and the data changes drastically over time, then the trained model will not be as effective and it will require constant training to ensure it's accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f53fc-cabb-4628-b150-8da5b9107e51",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Text classification steps <a name=\"steps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ee2943-f4b3-45b5-89a4-5a7437ae18c5",
   "metadata": {},
   "source": [
    "1. Data collection: gather your data with labeled examples\n",
    "\n",
    "2. Text pre-processing: clean your text data to remove noise, inconsistencies, remove stopwords, converting text to lowercase etc. This is a key step, probably as important as building the model itself. When done correctly, it can drastically improve model performance. \n",
    "\n",
    "   Check out this kaggle notebook that goes into detail on pre-processing: https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing/notebook\n",
    "   \n",
    "   Tokenization is also an important step in text pre-processing. See this article: https://www.datacamp.com/blog/what-is-tokenization\n",
    "\n",
    "\n",
    "\n",
    "3. Feature extraction: represent the text data as numerical features that the machine learning model can understand. (Machine learning models can't actually take text data as an input, they would not be able to make sense of it. Turning the features into numbers that the model can understand is a key step). \n",
    "\n",
    "   See this article on feature extraction techniques: https://www.analyticsvidhya.com/blog/2022/05/a-complete-guide-on-feature-extraction-techniques/\n",
    "\n",
    "4. Model training: train the machine model on labeled data. There are many techniques that can be used such as Naive Bayes, Support Vector Machines (SVM), neural networks and more. \n",
    "\n",
    "5. Model evaluation: assess the performance of the trained model on a separate set of data (called test set) that has not been used during training. \n",
    "\n",
    "Once you are happy with how the model is performing, you can save the model for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b82f15b-6298-497b-a872-58c414eef448",
   "metadata": {},
   "source": [
    "# 4. Text classification example - spam text message data from kaggle <a name=\"example_start\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b47ea2-13ed-4746-8dcf-08f996076293",
   "metadata": {},
   "source": [
    "Going to use data from kaggle. It contains two columns: a column with some messages in them and a column that details whether the message is \"ham\" or spam. \n",
    "\n",
    "Sourced from here: https://www.kaggle.com/datasets/team-ai/spam-text-message-classification?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f29834-0348-4f02-a024-4e70b01fe4ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages first\n",
    "\n",
    "import os\n",
    "\n",
    "# data manipulation packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data visualisation packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# regular expressions package\n",
    "import re\n",
    "\n",
    "# NLP packages & functions\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd39ec-3a1e-4e38-a303-2e171e161d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the current working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13f828-9de0-498e-bb42-06b927c2ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/SPAM text message data kaggle.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fb0b7-2f7e-46df-9080-ca0046cdd5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check whether there are any rows with NA. If there are, let's remove them.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3e691-4fce-434c-8598-ef3c5b660e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check whether there are any duplicated columns\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77761b3f-606c-49a1-b39a-2533b0ff915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are duplicates in the data, let's get rid of them\n",
    "df = df.drop_duplicates(keep = 'first')\n",
    "\n",
    "# Let's check the shape of the data after removing duplicates\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c8d04-bd07-44c7-84f9-cff229b76b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the category column\n",
    "sns.countplot(x=\"Category\", data = df, hue = \"Category\")\n",
    "\n",
    "# You can see that the data is unbalanced, with the majority of messages being ham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9527894-fe27-485b-bda1-9044ea48cb1b",
   "metadata": {},
   "source": [
    "## 4.1. Text Pre-Processing <a name=\"text_pre_processing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e249495-1eb2-4f26-bfc3-696fd35ee53b",
   "metadata": {},
   "source": [
    "Here, we're trying to standardise the data as much as possible by doing things such as removing hyperlinks, emojis and implementing classic NLP cleaning techniques (stemming, etc). These help us in training better models\n",
    "\n",
    "In this script, we're going to do each step one by one, but usually, you would write one big function that does all the steps in one go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805aa42-aec8-44c8-9acd-ae6c50e5ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a column that turns the Category column into a binary one\n",
    "df[\"spam\"] = df[\"Category\"].apply(lambda x: 1 if x == \"spam\" else 0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febda150-9786-4f7e-bec1-b18237ab6d15",
   "metadata": {},
   "source": [
    "### 4.1.1. Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c4022-fce2-4785-b29e-cb14718b5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that makes all the text in lowercase\n",
    "def to_lowercase(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Now apply the function to the text column\n",
    "df[\"cleaned_message\"] = df[\"Message\"].apply(to_lowercase)\n",
    "\n",
    "# print the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25017b-14b0-4747-af7d-3319049b195b",
   "metadata": {},
   "source": [
    "### 4.1.2. Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a75da-e8c0-4782-ba5b-9efcfa4c8fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The string package contains a function that outputs all punctuation, we're going to use it in our function\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f01711-f66d-4c00-aed5-f660b35689b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that removes punctuation from any given text\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# Apply to the column of interest\n",
    "df[\"cleaned_message\"] = df[\"cleaned_message\"].apply(remove_punctuation)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154d3ac-e160-4b4e-9d84-80df008bff9d",
   "metadata": {},
   "source": [
    "### 4.1.3. Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a4b4f-0ed6-4507-ba47-b8380c4ec49c",
   "metadata": {},
   "source": [
    "Stop words are a set of commonly used words in a language. Examples of stop words in English are âa,â âthe,â âis,â âare,â etc. \n",
    "\n",
    "Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so widely used that they carry very little useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ffbc3-952b-4bb1-9cd1-fb3bd89a2150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The package nltk already contains a list of stopwords inside it that we can call upon. Makes life easier when wanting to remove all stopwords from text.\n",
    "# You can call the function to print the list. I am attaching \", \".join() to the function to paste all of them together rather than printing a long list.\n",
    "# When calling on the function, you have to specify the language\n",
    "\", \".join(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba9f8a-999c-44bf-8d81-582dc36acf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a name to the stopwords first\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Create a function that removes stopwords\n",
    "def remove_stopwords(text):\n",
    "    \n",
    "    # create an empty list\n",
    "    list = []\n",
    "    \n",
    "    # tokenize the text\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # loop across each tokenized text and remove stopwords\n",
    "    for word in tokenized_text:\n",
    "        if word not in stopwords:\n",
    "            list.append(word)\n",
    "        \n",
    "    return \" \".join(list)\n",
    "    \n",
    "# Apply the function to the data\n",
    "df[\"cleaned_message\"] = df[\"cleaned_message\"].apply(remove_stopwords)\n",
    "\n",
    "#print\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07702a8f-3161-489b-9388-8bf6e6a12015",
   "metadata": {},
   "source": [
    "### 4.1.4. Removing frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d26f7-b3a4-4caa-acde-9171d2a691c0",
   "metadata": {},
   "source": [
    "Here, we are talking about removing words that are NOT stopwords. Removing frequent words is something that you might want to do on a case by case basis, depending on the project you're working on. \n",
    "\n",
    "Essentially, we're trying to remove words that occur very frequently in the corpus that might not be informative for the specific classification task at hand.\n",
    "\n",
    "You may consider doing this if certain words occur very frequently across all documents in your corpus. They might not carry much discriminatory power so by removing highly frequent non-stop words, you can help the model focus on the more distinctive and discriminative terms that are more likely to capture the semantics relevant to the classification task.\n",
    "\n",
    "\n",
    "It's up to you to determine whether they have value or not. In some cases, removing highly frequent non-stop words may improve performance, while in others, it might not make a significant difference or could even degrade performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d107bb-5ca9-4158-8e48-8b0803fac0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's check which words occur more frequently\n",
    "# Initialise a counter\n",
    "from collections import Counter\n",
    "frequent_words = Counter()\n",
    "\n",
    "# Check the words\n",
    "for text in df[\"cleaned_message\"]:\n",
    "    for word in text.split():\n",
    "        frequent_words[word] += 1\n",
    "        \n",
    " #print the 10 most common words       \n",
    "frequent_words.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c85b3b-803b-4341-8821-77016307a7e8",
   "metadata": {},
   "source": [
    "In this case, it might actually be worth removing some frequent words - they are basically stopwords! But because they are misspelled (u instead of you, dont instead don't etc.), they were not removed in the previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a0c23-3ebd-4363-a6c6-b9d38bc0417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just select the stopwords and remove them\n",
    "remove_words = [frequent_words.most_common(10)[0][0], frequent_words.most_common(10)[2][0], frequent_words.most_common(10)[5][0], frequent_words.most_common(10)[7][0]]\n",
    "   \n",
    "print(remove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eacd67-250e-4708-b8b8-ac0256cc92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove the words\n",
    "def remove_frequent_words(text):\n",
    "    return \" \".join([word for word in text.split() if word not in remove_words])\n",
    "\n",
    "df[\"cleaned_message\"] = df[\"cleaned_message\"].apply(remove_frequent_words)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a3c9e-f2db-4d54-bb30-53f03bda7c1e",
   "metadata": {},
   "source": [
    "You could do the same with rare words if you wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36d3f1-1cba-44ed-b03e-cc9bf020be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words.most_common()[:-10:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a252095-e08d-49f1-8c80-6fbcc257dacf",
   "metadata": {},
   "source": [
    "### 4.1.5. Removing special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69fbc6-88c7-4c0a-b3d7-1beca474f971",
   "metadata": {},
   "source": [
    "Remove any special characters (that hasn't already been removed by the previous steps). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b013c-dc1c-4471-a31e-7fde0e32fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function\n",
    "def remove_special_chars(text):\n",
    "    # The regular expression here means except anything between a to z, A to Z and 0 to 9. So anything that fulfills that condition will be replaced with a space\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\",\" \", text)\n",
    "    # replace any extra whitespaces with a single one\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "# Apply it\n",
    "df[\"cleaned_message\"] = df[\"cleaned_message\"].apply(remove_special_chars)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90506e54-dad5-48b3-9a84-198ddee2717c",
   "metadata": {},
   "source": [
    "### 4.1.6. Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8679c-60f6-4eaa-b1be-f6c2b486f93d",
   "metadata": {},
   "source": [
    "Stemming, in Natural Language Processing (NLP), refers to the process of reducing a word to its word stem that affixes to suffixes and prefixes or the roots.\n",
    "\n",
    "For example, the words âprogramming,â âprogrammer,â and âprogramsâ can all be reduced down to the common word stem âprogram.â In other words, âprogramâ can be used as a synonym for the prior three inflection words.\n",
    "\n",
    "Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575ba4d-b9c9-45da-a12d-de678f749097",
   "metadata": {},
   "source": [
    "Doing either stemming or lemmatization (or both!!) is a key step in text pre-processing. \n",
    "\n",
    "Read more about the two here: https://www.datacamp.com/tutorial/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a2803e-cd41-436b-a01d-0bf2758f05e2",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665889f-4f59-4b42-a098-9e87d537bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the required function\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create an instance \n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Create a function that does the stemming\n",
    "def stemmer(text):\n",
    "    \n",
    "    # Tokenization using NLTK\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # empty list\n",
    "    list = []\n",
    "    \n",
    "    # loop across each text\n",
    "    for text in tokenized_text:\n",
    "        list.append(ps.stem(text))\n",
    "        \n",
    "    return \" \".join(list)\n",
    "\n",
    "# Apply the function\n",
    "df[\"cleaned_message_stemmed\"] = df[\"cleaned_message\"].apply(stemmer)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98259b-9a0c-4d18-9787-abf12f7cc779",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37d5eb-aa93-492d-a7f4-96767fb6b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The steps are quite similar to above\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create an instance\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "# Function\n",
    "def lemmatize(text):\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # empty list\n",
    "    list = []\n",
    "    \n",
    "    # loop across each text\n",
    "    for text in tokenized_text:\n",
    "        list.append(lemmatizer.lemmatize(text))\n",
    "        \n",
    "    return \" \".join(list)\n",
    "\n",
    "# Apply the function\n",
    "df[\"cleaned_message_lemmatized\"] = df[\"cleaned_message\"].apply(lemmatize)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca4b73-192c-408d-b2e3-e3ddcbe81c90",
   "metadata": {},
   "source": [
    "Going forward, we'll use the stemmed column for simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c4029-2976-4f78-8a8f-a3dae5eaf116",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1.7. Removing URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979462b-a31f-4934-a108-49d68e4dca7f",
   "metadata": {},
   "source": [
    "URLs don't contribute to text analysis, they are just noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc119a50-25c7-4c05-a52d-162d5096d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(\"r'https?://\\S+www\\.\\/S+\", \"\", text)\n",
    "\n",
    "df[\"cleaned_message_stemmed\"] = df[\"cleaned_message_stemmed\"].apply(remove_url)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b983c61a-ef47-4d70-9ff0-d84a02423daf",
   "metadata": {},
   "source": [
    "These include most of the steps that you'd usually perform in text pre-processing. Depending on your data, you may have to perform other things. \n",
    "\n",
    "I've done it step by step so you are able to compare how each function impacts the text, but in standard practice, you'd usually combine everything into one function and apply it to the text. Makes it faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac854e9-1040-4a31-a2de-efe87dc0349e",
   "metadata": {},
   "source": [
    "## 4.2. Feature extraction <a name=\"feature_extraction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3515cf-bb97-436d-b23b-200dbd6f7447",
   "metadata": {},
   "source": [
    "As mentioned earlier, feature extraction is a step in which you convert the raw text data into numerical inputs that the model can understand. \n",
    "\n",
    "There are many techniques that you can perform, for example: \n",
    "1. Bag of words (BOW). Read about it here: https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "2. Term Frequency - Inverse Document Frequency (TF-IDF). Read more about it here: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/\n",
    "3. Word embeddings (word2vec, GloVe). Read more about it here: https://www.turing.com/kb/guide-on-word-embeddings-in-nlp\n",
    "\n",
    "I've just listed the more popular ones, but there are more. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cea93b-705c-4d00-b430-8fcdd1bc2c22",
   "metadata": {},
   "source": [
    "Before performing TF-IDF, we're going to split the data into training and test set first. \n",
    "\n",
    "test_size = 0.3 means that we're splitting the data into 70% training data and 30% test data. We [split the data](https://www.linkedin.com/pulse/why-do-we-need-data-splitting-utkarsh-sharma) to avoid overfitting the model, to ensure that the model will perform reasonably well on unseen data. Using a 70%-30% or 80%-20% split is common practice as that empirically, these tend to get the best results\n",
    "\n",
    "However, there's no hard rule here. It depends on your project as well. You want to ensure that you're training the model and evaluating it on enough data. So sample size of both sets should be big enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db442090-87cd-43df-8fbc-b4cdd3dc42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"cleaned_message_stemmed\"]\n",
    "y = df[\"spam\"]\n",
    "\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff497a1f-a1e3-4aa2-aece-3a348e6d9d25",
   "metadata": {},
   "source": [
    "In this example, we're going to use TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4352cc-76e3-4e41-86d8-afd0cd0af965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create an instance of the vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Now you want to pass the text through the vectorizer. fit_transform lets the vectorizer learn the vocabulary and the IDF and return a document-term matrix. \n",
    "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92748268-9dfe-4dc4-bb7a-32f9a0651c5b",
   "metadata": {},
   "source": [
    "There are actually many arguments that you could pass through the TfidfVectorizer() function that may be useful to your problem, most common ones being max_features, min_df or max_df etc.\n",
    "\n",
    "Check out the documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e9f8b-9169-4dc3-b67a-ed070b4dd4d6",
   "metadata": {},
   "source": [
    "## 4.3. Model training <a name=\"model_training\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd50306-1a26-4b14-a857-8d377cc37cd0",
   "metadata": {},
   "source": [
    "Model training in machine learning refers to the process of teaching a machine learning model to recognize patterns and make predictions from data. \n",
    "\n",
    "In a supervised learning setting, during the training phase, the model learns from a labeled dataset, where input data (features) are paired with corresponding output labels (targets or responses)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e1559-8569-4e04-a835-c09a518ed8a9",
   "metadata": {},
   "source": [
    "In this script, we're going to try out 3 classifiers: multinomial naive Bayes, random forest and linear support vector classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f64af-1843-40fb-90d1-319d5a0012f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238a51a-ba72-4fac-962e-e3a2319aa22f",
   "metadata": {},
   "source": [
    "### 4.3.1. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be41e87-7b7a-4df1-902d-7762aa9a2943",
   "metadata": {},
   "source": [
    "Article explaining how MNB works: https://www.upgrad.com/blog/multinomial-naive-bayes-explained/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a87e9d5-5a70-40ac-bedc-ad4a25d25a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the model up\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "# Fit the model to the vectorised training set (this is where the actual machine learning is happening)\n",
    "MNB.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Now that the model has learnt from the training data, let it make predictions on the test set (which it hasn't seen before) and see how accurate it is against the true labels.\n",
    "y_prediction_MNB = MNB.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate the probability estimates for each class label for a given input sample.\n",
    "y_probability_MNB = MNB.predict_proba(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83de0e-e7c6-4b2f-ab04-189c55e4d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the prediction and the probability back to the dataset with the actuals\n",
    "df_test = pd.DataFrame(X_test)\n",
    "df_test[\"Actual\"] = y_test\n",
    "df_test[\"MNB_prediction\"] = y_prediction_MNB\n",
    "df_test[\"MNB_probability\"] = y_probability_MNB[:, 0]\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3b0d8-379b-4443-87bb-15bb56bd5543",
   "metadata": {},
   "source": [
    "### 4.3.2. Random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e6dd0d-f8eb-404b-9ec0-56c27e2f20fc",
   "metadata": {},
   "source": [
    "Nice article from Datacamp on random forest with an explanation of how the algorithm works: https://www.datacamp.com/tutorial/random-forests-classifier-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38830705-6b91-482c-a90c-85c0ed29ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "# You can choose the number of trees in the classifiers with the n_estimators argument. The default is 100 so I'll just leave it as that. \n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_prediction_rf = rf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate the probabilities of each category\n",
    "y_probability_rf = rf.predict_proba(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac52aac-8f7a-4af0-8306-ce216dc33c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding prediction to the data\n",
    "df_test[\"rf_prediction\"] = y_prediction_rf\n",
    "df_test[\"rf_probability\"] = y_probability_rf[:, 0]\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a7faf-2e31-4cb9-9b7c-fa3f54b42d85",
   "metadata": {},
   "source": [
    "### 4.3.3. Linear SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc5186-4fb1-434a-9a25-cd9314d6d339",
   "metadata": {},
   "source": [
    "Article that explains how SVMs work: https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6165c68-7a77-4fe5-94f1-cca250809fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "LSVC = LinearSVC()\n",
    "\n",
    "# Fit the model\n",
    "LSVC.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_prediction_LSVC = LSVC.predict(X_test_tfidf)\n",
    "\n",
    "# LinearSVC is a linear classifier that directly learns decision boundaries without providing probability estimates.\n",
    "# So in this case, there's no predict_proba() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88974dc9-621d-489b-9fdf-ed528a4cd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prediction to the df\n",
    "df_test[\"LSVC_prediction\"] = y_prediction_LSVC\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af195353-ed43-458e-90e6-f5866695f0c3",
   "metadata": {},
   "source": [
    "## 4.4. Model evaluation <a name=\"model_evaluation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e69576-b75a-4ae1-b16b-cfe1dad4186f",
   "metadata": {},
   "source": [
    "### 4.4.1. Accuracy, precision and recall <a name=\"metrics_explained\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0223d-350d-4380-9a59-51405507a7fb",
   "metadata": {},
   "source": [
    "Accuracy, precision and recall are three metrics used to evaluate a machine learning model's performance. \n",
    "\n",
    "**Accuracy is the ratio of correctly classified instances to the total instances in the dataset.** It is a good measure when the classes are balanced, but it can be misleading if there is a class imbalance, as a model might achieve high accuracy by simply predicting the majority class.\n",
    "\n",
    "**Precision measures the proportion of true positive predictions (correctly identified positive instances) out of all positive predictions made.** It is useful when it is important to minimize false positives, such as in spam detection or medical diagnosis where false positives could have serious consequences.\n",
    "\n",
    "**Recall (also known as sensitivity or true positive rate) measures the proportion of true positive instances that were actually identified correctly from all the actual positive samples in the dataset.** It is important when it is critical to find all positive instances, such as in fraud detection where missing a fraudulent transaction could be costly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8538c-f024-44b6-ac62-6706813a011d",
   "metadata": {},
   "source": [
    "When deciding which metric to use, consider the following:\n",
    "\n",
    "Use accuracy when the classes are balanced and the cost of false positives and false negatives is similar.\n",
    "\n",
    "Use precision when it is crucial to minimize false positives, even if it means missing some true positives.\n",
    "\n",
    "Use recall when it is essential to find all positive instances, even if it means increasing the number of false positives.\n",
    "\n",
    "Depending on your problem, you may care more about one metric than another.\n",
    "\n",
    "See these sources for more on accuracy, precision and recall: \n",
    "\n",
    "https://www.kimberlyfessel.com/mathematics/data/accuracy-precision-recall/\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/classification/accuracy\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903be8bc-00b2-46df-a70c-832fb3c2fa86",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681ef24-4606-4791-986e-de335d2f88e1",
   "metadata": {},
   "source": [
    "To visualise accuracy, precision and recall, you could create a confusion matrix. It may give you a better insight into each models. \n",
    "\n",
    "This article from DataCamp gives a good introduction to confusion matrices:\n",
    "\n",
    "https://www.datacamp.com/tutorial/what-is-a-confusion-matrix-in-machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78359f5a-fbb2-418c-a4d6-364ff3907d31",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cff61-a795-44ee-955f-78c68d465d46",
   "metadata": {},
   "source": [
    "Ideally, when looking at precision and recall, you'd want your classifier to have high precision and high recall. This is not always possible, and there's also usually a trade-off between the two: trying to improve one comes at the cost of the other. \n",
    "\n",
    "There are instances in which it's desirable to have both high precision and high recall - this is where the F1 score comes in. \n",
    "\n",
    "The F1 score is calculated as the harmonic mean of the precision and recall of a classification model. Both metrics equally contribute to the score and the effect of smaller values is enhanced to ensure the score correctly reflects this. \n",
    "\n",
    "The score ranges from 0 to 1, with 1 representing a model that can accurately classifies each observation into its correct class and 0 representing a model that cannot categorise any observation correctly. \n",
    "\n",
    "A high F1 score indicates a well balanced model performance, with high precision and high recall. \n",
    "\n",
    "Read more about F1 score here: \n",
    "\n",
    "https://encord.com/blog/f1-score-in-machine-learning/#:~:text=The%20F1%20score%20or%20F,the%20reliability%20of%20a%20model.\n",
    "\n",
    "https://arize.com/blog-course/f1-score/\n",
    "\n",
    "https://deepai.org/machine-learning-glossary-and-terms/f-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f9d2d-5858-40ab-aeb4-286ef4becd5b",
   "metadata": {},
   "source": [
    "Now, let's calculate the three metrics for each of the models we've used in training and visualise the confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb5df3-b51d-40e5-a326-195e189da12f",
   "metadata": {},
   "source": [
    "### 4.4.2. Multinomial Naive Bayes evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa3271-9792-40d8-8fff-b90347868f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scores for MNB model\n",
    "accuracy_MNB = accuracy_score(y_test, y_prediction_MNB)\n",
    "precision_MNB = precision_score(y_test, y_prediction_MNB)\n",
    "recall_MNB = recall_score(y_test, y_prediction_MNB)\n",
    "\n",
    "print(f'Accuracy of the model: {accuracy_MNB}')\n",
    "print(f'Precision Score of the model: {precision_MNB}')\n",
    "print(f'Recall Score of the model: {recall_MNB}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58970d46-4fbc-4f6b-9199-ad64a4d5255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for MNB model\n",
    "\n",
    "# Create the confusion matrix\n",
    "cf_MNB = confusion_matrix(df_test.Actual, df_test.MNB_prediction)\n",
    "\n",
    "# Print the matrix\n",
    "cf_MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29fc274-b14c-4f48-8714-93f5ef33009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice above how the result is quite literally a matrix. \n",
    "# Our aim, however, is to create a plot with labels that gives more clarity and is easier to understand\n",
    "\n",
    "# Plot the confusion matrix\n",
    "sns.heatmap(cf_MNB, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('Actual label')\n",
    "plt.title('Multinomial Naive Bayes Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a65ce-9e9d-4044-94f9-99565b6c3341",
   "metadata": {},
   "source": [
    "Let's recall the definition of precision first: it measures the proportion of true positive predictions (correctly identified positive instances) out of all positive predictions made.\n",
    "\n",
    "If you wanted to calculate it manually just from the confusion matrix, you'd do 125/(125+0) = 1. \n",
    "\n",
    "This is exactly the same as the precision score of 1 calculated above using the precision_score() function. There were no instances in which the model incorrectly classified a genuine messages as spam (no false positives). Hence why the top right in the confusion matrix is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9a04f-6658-43c7-90ed-4f3a56723a8e",
   "metadata": {},
   "source": [
    "Let's do the same for recall. Recall measures the proportion of true positive instances that were actually identified correctly from all the actual positive samples in the dataset.\n",
    "\n",
    "Calculating it manually from the confusion matrix, we would do: 125/(125+72) = 0.63.\n",
    "\n",
    "There were 72 instances where spam messages were categorised as genuine messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed75790-b8a6-4f7c-8a27-050393b19209",
   "metadata": {},
   "source": [
    "In this example specifically, we care more about precision: we want to avoid miss-classifying genuine messages into spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce505ed3-af28-4344-b23e-a920fba3cefe",
   "metadata": {},
   "source": [
    "### 4.4.3. Random Forest Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002b2061-c905-4820-86bb-7da81156e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cf_rf = confusion_matrix(df_test.Actual, df_test.rf_prediction)\n",
    "\n",
    "# Create plot for matrix\n",
    "sns.heatmap(cf_rf, annot=True, fmt=\"d\", cmap=\"Reds\", \n",
    "            xticklabels = [\"Predicted negative\", \"Predicted positive\"],\n",
    "            yticklabels = [\"Actual negative\", \"Actual positive\"])\n",
    "\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"Actual label\")\n",
    "plt.title(\"Random forest confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics for random forest\n",
    "accuracy_rf = accuracy_score(y_test, y_prediction_rf)\n",
    "precision_rf = precision_score(y_test, y_prediction_rf)\n",
    "recall_rf = recall_score(y_test, y_prediction_rf)\n",
    "\n",
    "print(f'Accuracy of the model: {accuracy_rf}')\n",
    "print(f'Precision Score of the model: {precision_rf}')\n",
    "print(f'Recall Score of the model: {recall_rf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59baf5b-1cb1-4857-a9bd-3e7a8bbd7741",
   "metadata": {},
   "source": [
    "### 4.4.4. Linear Support Vector Classifier Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9b1b4-d08d-4b4e-83b4-8f8716e8840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cf_lsvc = confusion_matrix(df_test.Actual, df_test.LSVC_prediction)\n",
    "\n",
    "# Create plot for confusion matrix\n",
    "sns.heatmap(cf_lsvc, annot = True, fmt = \"d\", cmap = \"Blues\",\n",
    "            xticklabels = [\"Predicted negative\", \"Predicted positive\"],\n",
    "            yticklabels = [\"Actual negative\", \"Actual positive\"])\n",
    "\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"Actual label\")\n",
    "plt.title(\"LSVC confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics for LSVC\n",
    "accuracy_LSVC = accuracy_score(y_test, y_prediction_LSVC)\n",
    "precision_LSVC = precision_score(y_test, y_prediction_LSVC)\n",
    "recall_LSVC = recall_score(y_test, y_prediction_LSVC)\n",
    "\n",
    "print(f'Accuracy of the model: {accuracy_LSVC}')\n",
    "print(f'Precision Score of the model: {precision_LSVC}')\n",
    "print(f'Recall Score of the model: {recall_LSVC}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b82b4-eb44-4b7c-a0b7-1ad858255e78",
   "metadata": {},
   "source": [
    "## 5. Some further notes <a name=\"further_notes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573234a5-92eb-42c2-b32a-6c56424a76ce",
   "metadata": {},
   "source": [
    "### 5.1. Balanced datasets <a name=\"balanced_datasets\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992aad0-f138-473f-b70b-cf3041becc19",
   "metadata": {},
   "source": [
    "A balanced dataset is one in which the number of samples for each class is roughly the same. \n",
    "\n",
    "In a binary classification problem, for instance, a balanced dataset would have an equal number of instances for both classes. \n",
    "\n",
    "This is important because in machine learning, a model trained on an imbalanced dataset can become biased towards the majority class, which can lead to poor performance when predicting the minority class.\n",
    "\n",
    "Balancing a dataset can help prevent such bias and improve the model's ability to generalize to new, unseen data.\n",
    "\n",
    "Check this article out on some ways you can deal with an unbalanced dataset: https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e43caf-dda4-42bf-86b0-c00cc9b702b9",
   "metadata": {},
   "source": [
    "### 5.2. Exploratory Data Analysis <a name=\"EDA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d962e50-96cc-4c41-bc7a-89f00ae7bdb8",
   "metadata": {},
   "source": [
    "One step that is usually performed, but hasn't been done here is that of explatoratory data analysis. \n",
    "\n",
    "Usually, you would want to properly explore the text data and understand more about it. What words are common? How long are the words? Etc. Anything that would give you an idea on the text you're working on.\n",
    "\n",
    "Check out this article on performing EDA: https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c643d63-2fbc-4ffd-b678-8a974ff7ac23",
   "metadata": {},
   "source": [
    "# 6. Further Resources <a name=\"resources\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfaa882-387d-4634-ad15-0b8794b54ef0",
   "metadata": {},
   "source": [
    "Here are some resources you can use:\n",
    "\n",
    "NLP Datacamp course in Python: https://www.datacamp.com/tracks/natural-language-processing-in-python\n",
    "\n",
    "NLP Datacamp course in R: https://www.datacamp.com/courses/introduction-to-natural-language-processing-in-r\n",
    "\n",
    "Textmining with R textbook: https://www.tidytextmining.com/\n",
    "\n",
    "Google's Introduction to Machine Learning crash course: https://developers.google.com/machine-learning/crash-course/ml-intro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-code-examples",
   "language": "python",
   "name": "nlp-code-examples-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
