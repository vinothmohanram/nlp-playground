{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the present work we were commissioned by the Children, Young People and Education (CAYE)\n",
    "##### Committee to explore data science techniques to carry out an independent analysis of 650 responses\n",
    "##### to a consultation gathering opinions on a recent Welsh Government Bill-the Children (Abolition of\n",
    "##### Defence of Reasonable Punishment) (Wales) Bill (“the Bill”). \n",
    "##### This notebook describes the methods used in our analysis.\n",
    "##### More details on the background of the cosultation can be found here:\n",
    "http://senedd.assembly.wales/mgIssueHistoryHome.aspx?IId=24674\n",
    "    \n",
    "\n",
    "##### The techniques used in our analysis include  word collocation and tf-idf for salient phrases detection\n",
    "##### We also pulled out the context surrounding the salient phrases\n",
    "##### Author: Dr Chaitanya Joshi\n",
    "ONS Data Science Campus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1:Import the necessary libraries\n",
    "\n",
    "##### There are various online resources available to analyse free text data using NLP techniques. Some of the concepts used in the notebook are covered here:\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "\n",
    "https://buhrmann.github.io/tfidf-analysis.html\n",
    "\n",
    "https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "\n",
    "import operator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "import os.path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from contractions import contractions_dict\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "\n",
    "import scipy.stats as ss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Preliminaries\n",
    "\n",
    "##### Update the stop-words list\n",
    "##### Specify the required font size etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Stemming and Lemmatizer objects\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','could','would','should'])\n",
    "\n",
    "                \n",
    "del_list=['do','does','not','no','dont','wont']\n",
    "\n",
    "\n",
    "stop_words=[e for e in stop_words if e not in del_list]\n",
    "\n",
    "\n",
    "#Default font is not always the optimal choice\n",
    "fnt_sz=20\n",
    "\n",
    "fnt_sz_title=25\n",
    "\n",
    "fnt_sz_tk=15\n",
    "\n",
    "fnt_sz_tk_big=12\n",
    "\n",
    "fnt_sz_legend_big=12 \n",
    "fig_siz=(10,5)\n",
    "fig_siz_big=(15,10)\n",
    "\n",
    "#Destination to storing results\n",
    "path_to_results='./results/figures/'\n",
    "path_to_file='./results/'\n",
    "\n",
    "#One can also tweak the font type etc in matplotlib\n",
    "plt.style.use('fivethirtyeight')#seaborn-white')\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "#plt.rcParams['axes.tickweight'] = 'bold'\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 12\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Load the dataset\n",
    "\n",
    "##### Specify the required columns\n",
    "##### load the dataset\n",
    "##### Perform initial data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data:\n",
    "\n",
    "# The following needs to be tweaked if the dataset has different columns\n",
    "sep='+'*100\n",
    "consultn_data=pd.read_pickle(path_to_file+'consultn_data_complete.pl')\n",
    "\n",
    "print(consultn_data['UserType'].value_counts())\n",
    "print(sep)\n",
    "print('Total number of responses received is {}'.format(consultn_data['UserType'].shape[0]))\n",
    "print(sep)\n",
    "print('Number of responses in support or otherwise for the Bill')\n",
    "print(pd.DataFrame(consultn_data.groupby(['UserType','BillSupport'])['BillSupport'].apply(len)))\n",
    "\n",
    "\n",
    "reduced_columns=['Translation', 'ProfPersonal','Q2 Are you over 13 years old?',\\\n",
    "                 'Postcode-Personal', 'WhatLangSubmit-personal', 'Are you represening an Org',\\\n",
    "                 'UserType', 'Q6 Organisation - Eng', 'BillSupport',\\\n",
    "                 'Q10 12 Please outline your reasons for your answer to question',\\\n",
    "                 'Q11 13 Do you think there is a need for legislation to deliver',\\\n",
    "                 'Q12 21 Do you have any comments about any potential barriers t',\\\n",
    "                 'Q13 22 Do you think the Bill takes account of these potential ',\\\n",
    "                 'Q14 31 Do you think there are there any unintended consequence',\\\n",
    "                 'Q15 41 Financial Implications', 'Q16 51 Other Comments']\n",
    "\n",
    "\n",
    "smaller_colmns=[ 'Postcode-Personal','UserType', 'BillSupport',\\\n",
    "                 'Q10 12 Please outline your reasons for your answer to question',\\\n",
    "                 'Q11 13 Do you think there is a need for legislation to deliver',\\\n",
    "                 'Q12 21 Do you have any comments about any potential barriers t',\\\n",
    "                 'Q13 22 Do you think the Bill takes account of these potential ',\\\n",
    "                 'Q14 31 Do you think there are there any unintended consequence',\\\n",
    "                 'Q15 41 Financial Implications', 'Q16 51 Other Comments']\n",
    "\n",
    "\n",
    "\n",
    "df_responses_type=pd.DataFrame(consultn_data.groupby(['BillSupport','UserType'])['BillSupport'].apply(len))\n",
    "\n",
    "df_responses_type=df_responses_type.rename(columns={'BillSupport':'Number of responses'})\n",
    "df_responses_type=df_responses_type.reset_index()\n",
    "\n",
    "df_responses_type['Number of responses(%)']=np.round(100*df_responses_type['Number of responses']/\\\n",
    "df_responses_type['Number of responses'].sum(),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.1: Overall support or otherwise for the Bill (cumulative for different user types)\n",
    "##### Support for the Bill for different user types\n",
    "##### The following should be tweaked to allow for different column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses_combined_alltype=df_responses_type.groupby('BillSupport')['Number of responses','Number of responses(%)'].apply(sum).\\\n",
    "sort_values(by='Number of responses',ascending=False).reset_index()\n",
    "\n",
    "\n",
    "cmap = plt.get_cmap('Spectral')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 4)]\n",
    "own_palette = sns.color_palette(colors)\n",
    "plt.figure(figsize=fig_siz_big)\n",
    "\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "sns.set_color_codes('dark')\n",
    "ax=sns.barplot(x='BillSupport',y='Number of responses',data=df_responses_combined_alltype,palette=own_palette,ci=None)\n",
    "    \n",
    "plt.title('Overall support or otherwise for the bill', fontsize=fnt_sz_title) \n",
    "plt.xlabel('Type of response', fontsize = fnt_sz)\n",
    "plt.ylabel('Number of responses', fontsize = fnt_sz)\n",
    "plt.xticks(fontsize=fnt_sz)\n",
    "plt.yticks(fontsize=fnt_sz)\n",
    "plt.legend(fontsize=fnt_sz,loc='best')\n",
    "\n",
    "totals=[]\n",
    "\n",
    "for i in ax.patches:\n",
    "    totals.append(i.get_height())\n",
    "    \n",
    "totals=[x for x in totals  if ~np.isnan(x)]\n",
    "    \n",
    "\n",
    "    \n",
    "total=sum(totals)\n",
    "\n",
    "\n",
    "\n",
    "for i in ax.patches:\n",
    "    if ~np.isnan(i.get_height()):\n",
    "        ax.text(i.get_x()+0.1,i.get_height()+0.04,str(int(i.get_height())),ha = 'center',va='bottom',\\\n",
    "                fontsize=fnt_sz)\n",
    "        ax.text(i.get_x()+0.35,i.get_height()+0.095,'('+str(round((i.get_height()/total)*100,1))+'%)',ha = 'center', \\\n",
    "                va='bottom',fontsize=fnt_sz)\n",
    "\n",
    "# save the figure as .eps file\n",
    "#fig_multi_tags = ax.get_figure()\n",
    "plt.savefig(path_to_results+'response_type_overall_support_bar_chart.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df_responses_type.groupby(['UserType','BillSupport'])['Number of responses','Number of responses(%)'].\\\n",
    "apply(sum).sort_values(by='Number of responses(%)',ascending=False).\\\n",
    "to_html(path_to_results+'user_support.html')\n",
    "\n",
    "df_responses_combined_alltype.to_html(path_to_results+'all_responses_type.html')\n",
    "\n",
    "\n",
    "\n",
    "df_responses_combined_alltype['Number of responses(%)']=df_responses_combined_alltype['Number of responses(%)'].\\\n",
    "apply(lambda x: np.round(x,1))\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "#credit https://matplotlib.org/devdocs/gallery/pie_and_polar_charts/pie_demo2.html#sphx-glr-gallery-pie-and-polar-charts-pie-demo2-py\n",
    "\n",
    "\n",
    "# Make square figures and axes\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "\n",
    "labels=df_responses_combined_alltype['BillSupport']+str('-')+df_responses_combined_alltype['Number of responses(%)'].astype(str)+'%'\n",
    "patches, texts = plt.pie(df_responses_combined_alltype['Number of responses(%)'], colors=colors,startangle=90,shadow=True)\n",
    "plt.title('Overall support or otherwise for the bill', fontsize=fnt_sz_title) \n",
    "plt.legend(patches, labels, loc=\"top left\",fontsize=fnt_sz)\n",
    "plt.legend(patches,labels, bbox_to_anchor=(1.5,0.35), loc=\"bottom right\",bbox_transform=\\\n",
    "           plt.gcf().transFigure,fontsize=fnt_sz)\n",
    "plt.legend(patches,labels, bbox_to_anchor=(1.25,0.5), loc=\"upper right\",bbox_transform=\\\n",
    "           plt.gcf().transFigure,fontsize=fnt_sz)\n",
    "plt.axis('equal')\n",
    "plt.savefig(path_to_results+'response_type_overall_support_pie_chart.png',bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.2: Support or otherwise for the Bill for different user types (fraction of overall responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('Spectral')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 9)]\n",
    "\n",
    "own_palette = sns.color_palette(colors)\n",
    "\n",
    "plt.figure(figsize=fig_siz_big)\n",
    "ax=sns.barplot(x='BillSupport',y='Number of responses', hue=\"UserType\", data=df_responses_type,palette=own_palette)#,size=4, aspect=2) \n",
    "plt.title('Support or otherwise for the bill', fontsize=fnt_sz_title) \n",
    "plt.xlabel('Type of response', fontsize = fnt_sz)\n",
    "plt.ylabel('Number of responses', fontsize = fnt_sz)\n",
    "plt.xticks(fontsize=fnt_sz)\n",
    "plt.yticks(fontsize=fnt_sz)\n",
    "plt.legend(fontsize=fnt_sz,loc='best')\n",
    "    \n",
    "totals=[]\n",
    "\n",
    "for i in ax.patches:\n",
    "    totals.append(i.get_height())\n",
    "    \n",
    "totals=[x for x in totals  if ~np.isnan(x)]\n",
    "    \n",
    "\n",
    "    \n",
    "total=sum(totals)\n",
    "\n",
    "\n",
    "for i in ax.patches:\n",
    "    if ~np.isnan(i.get_height()):\n",
    "        ax.text(i.get_x()+0.1,i.get_height()+0.035,str(int(i.get_height())),ha = 'center', \\\n",
    "                va='bottom',fontsize=fnt_sz)\n",
    "        ax.text(i.get_x()+0.31,i.get_height()+0.05,'('+str(round((i.get_height()/total)*100,1))+'%)',ha = 'center', \\\n",
    "                va='bottom',fontsize=fnt_sz)\n",
    "plt.savefig(path_to_results+'response_type_support_bar_chart.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df_responses_type.groupby(['UserType','BillSupport'])['Number of responses','Number of responses(%)'].\\\n",
    "apply(sum).to_excel(path_to_results+'bill_support.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "df_responses_type_copy=df_responses_type.copy()\n",
    "df_responses_type_copy['Percentage']=np.round(100*df_responses_type_copy['Number of responses']\\\n",
    "                                              /np.sum(df_responses_type_copy['Number of responses']),1)\n",
    "                       \n",
    "df_responses_type_copy=df_responses_type_copy.sort_values(by='Percentage',ascending=False)\n",
    "\n",
    "def df_comb(x):\n",
    "    match=x[labl_1]+x[label_2]\n",
    "    return match\n",
    "\n",
    "df_responses_type_copy['Response_user']=df_responses_type_copy['UserType']+' ('+df_responses_type_copy['BillSupport']+')'\n",
    "\n",
    "\n",
    "df_responses_type_copy=df_responses_type_copy.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Make square figures and axes\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "labels=df_responses_type_copy['Response_user']+':'+df_responses_type_copy['Percentage'].astype(str)+'%'\n",
    "patches, texts = plt.pie(df_responses_type_copy['Percentage'], colors=colors, startangle=90, shadow=True)\n",
    "plt.title('Support or otherwise for the bill from Individuals, Organisations and Professionals', fontsize=fnt_sz_title) \n",
    "plt.legend(patches, labels, loc=\"top left\",fontsize=fnt_sz_title)\n",
    "plt.legend(patches,labels, bbox_to_anchor=(1.6,0.65), loc=\"upper right\",bbox_transform=\\\n",
    "           plt.gcf().transFigure,fontsize=fnt_sz_title)\n",
    "plt.axis('equal')\n",
    "plt.savefig(path_to_results+'response_type_support_pie_chart.png',bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Step 3.3: Support or otherwise for the Bill for different user types (showing distinct behaviour of responses received from Individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_support=pd.DataFrame(df_responses_type.groupby(['UserType','BillSupport'])['Number of responses'].apply(sum))\n",
    "\n",
    "\n",
    "df_user_support=df_user_support.reset_index()\n",
    "\n",
    "users_list=list(np.unique(df_user_support['UserType']))\n",
    "\n",
    "\n",
    "\n",
    "list_dfs=[df_user_support[df_user_support['UserType']==x] for x in users_list]\n",
    "\n",
    "list_dfs_sum=[np.sum(x['Number of responses']) for x in list_dfs]\n",
    "\n",
    "for x in range(len(list_dfs)):\n",
    "    \n",
    "    list_dfs[x]['Total']=list_dfs_sum[x]\n",
    "    \n",
    "    \n",
    "list_dfs_all=pd.concat(list_dfs)\n",
    "\n",
    "list_dfs_all['Percentage of responses(%)']=np.round(100*list_dfs_all['Number of responses']/list_dfs_all['Total'],1)\n",
    "\n",
    "list_dfs_all=list_dfs_all.groupby(['UserType','BillSupport'])\\\n",
    "['Number of responses','Percentage of responses(%)'].apply(sum)\n",
    "\n",
    "list_dfs_all.to_excel(path_to_results+'bill_support_each_strata.xlsx')\n",
    "\n",
    "list_dfs_all=list_dfs_all.reset_index()\n",
    "\n",
    "\n",
    "plt.figure(figsize=fig_siz_big)\n",
    "ax=sns.barplot(x='UserType',y='Number of responses', hue=\"BillSupport\", data=list_dfs_all,palette='husl')\n",
    "\n",
    "plt.title('Support or otherwise for the bill, by UserType', fontsize=fnt_sz_title)\n",
    "plt.xlabel('Support to the bill', fontsize = fnt_sz)\n",
    "plt.ylabel('Number of responses', fontsize = fnt_sz)\n",
    "plt.xticks(fontsize=fnt_sz)\n",
    "plt.yticks(fontsize=fnt_sz)\n",
    "plt.legend(fontsize=fnt_sz,loc='best')\n",
    "totals=[]\n",
    "\n",
    "for i in ax.patches:\n",
    "    totals.append(i.get_height())\n",
    "    \n",
    "totals=[x for x in totals  if ~np.isnan(x)]\n",
    "    \n",
    "\n",
    "    \n",
    "total=sum(totals)\n",
    "for i in ax.patches:\n",
    "    if ~np.isnan(i.get_height()):\n",
    "        ax.text(i.get_x()+0.1,i.get_height()+0.04,str(int(i.get_height())),\\\n",
    "                ha = 'center', va='bottom',fontsize=fnt_sz)\n",
    "        ax.text(i.get_x()+0.255,i.get_height()+0.095,'('+str(list(list_dfs_all[list_dfs_all['Number of responses']==i.get_height()]['Percentage of responses(%)'])[0])+'%)',ha = 'center', \\\n",
    "                                                            va='bottom',fontsize=fnt_sz)\n",
    "plt.savefig(path_to_results+'each_strata_bill_support_percent.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Step 3.4: Collating responses to all the questions based on support or otherwise for the Bill (ignoring the user types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ_respons=list(set(consultn_data['BillSupport']))\n",
    "\n",
    "#typ_respons=['No', 'Yes', 'Partly', \"Don't have a view\"]\n",
    "\n",
    "#Look at the responses to the Questions in light of their response whether they support the bill or not.\n",
    "\n",
    "consultn_data_types_list=[consultn_data[consultn_data['BillSupport']==x]\\\n",
    "                          for x in typ_respons]\n",
    "\n",
    "consultn_data_types_list_shrt=[]\n",
    "\n",
    "respons_12=[]\n",
    "respons_13=[]\n",
    "respons_21=[]\n",
    "respons_22=[]\n",
    "respons_31=[]\n",
    "respons_41=[]\n",
    "respons_51=[]\n",
    "\n",
    "for index in range(len(consultn_data_types_list)):\n",
    "    \n",
    "    list_columns=['BillSupport',\\\n",
    "                  'Q10 12 Please outline your reasons for your answer to question',\\\n",
    "                  'Q11 13 Do you think there is a need for legislation to deliver',\\\n",
    "                  'Q12 21 Do you have any comments about any potential barriers t',\\\n",
    "                  'Q13 22 Do you think the Bill takes account of these potential ',\\\n",
    "                  'Q14 31 Do you think there are there any unintended consequence',\\\n",
    "                  'Q15 41 Financial Implications', 'Q16 51 Other Comments']\n",
    "    consultn_data_types_list[index][list_columns]=consultn_data_types_list[index][list_columns].astype(str)\n",
    "    consultn_data_types_list_shrt.append(consultn_data_types_list[index][list_columns])\n",
    "    respons_12.append('/~~/'.join(consultn_data_types_list[index][list_columns[1]].values))\n",
    "    respons_13.append('/~~/'.join(consultn_data_types_list[index][list_columns[2]].values))\n",
    "    respons_21.append('/~~/'.join(consultn_data_types_list[index][list_columns[3]].values))\n",
    "    respons_22.append('/~~/'.join(consultn_data_types_list[index][list_columns[4]].values))\n",
    "    respons_31.append('/~~/'.join(consultn_data_types_list[index][list_columns[5]].values))\n",
    "    respons_41.append('/~~/'.join(consultn_data_types_list[index][list_columns[6]].values))\n",
    "    respons_51.append('/~~/'.join(consultn_data_types_list[index][list_columns[7]].values))\n",
    "\n",
    "all_responses=[respons_12,respons_13,respons_21,respons_22,respons_31,respons_41,respons_51]\n",
    "\n",
    "dict_abvn = {'ASAP':'as-soon-as-possible','asap': 'as-soon-as-possible', 'AFAIK': 'as-far-as-I-know',\\\n",
    "     'AFAIK': 'as-far-as-I-know','fta': 'free-trade-agreement','eu': 'european-union','EU': 'european-union',\\\n",
    "     'the us': 'united states','the US': 'united states','ukeu':'uk-and-eu','uk-eu':'uk-and-eu','UK-EU':'uk-and-eu',\\\n",
    "    'united states': 'united states','united kingdom': 'united kingdom','nhs':'national-health-service',\\\n",
    "            'NHS':'national-health-service','child on child violence':'child-on-child violence',\\\n",
    "             'child on child':'child-on-child','child child violence':'child-on-child violence',\\\n",
    "             'child child abuse':'child-on-child abuse'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.5: Perform text cleaning on responses to all the questions based on support or otherwise for the Bill (ignoring the user types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "numb_support_bill=len(list(set(df_responses_type['BillSupport'])))\n",
    "if list(set([len(all_responses[x])==numb_support_bill for x in range(len(all_responses))]))[0]:\n",
    "    print('OK')\n",
    "else:\n",
    "    print('not-OK')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#expand contractions\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=contractions_dict):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "pattern=re.compile('^\\s+|\\s*,\\s*|\\s+$')\n",
    "\n",
    "def remove_spaces(y):\n",
    "    chck=[x for x in pattern.split(y) if x]\n",
    "    return chck\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Questn_0='Please outline your reasons for your support of the principles of the children abolition \\\n",
    "of defence of reasonable punishment wales bill'\n",
    "\n",
    "\n",
    "\n",
    "Questn_1='Do you think there is a need for legislation to deliver what children abolition \\\n",
    "of defence of reasonable punishment wales bill is trying to achieve'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Questn_2='Do you have any comments about any potential barriers to implementing the\\\n",
    "children abolition of defence of reasonable punishment wales bill'\n",
    "\n",
    "\n",
    "Questn_3='Do you think the children abolition of defence of reasonable punishment wales bill takes account of \\\n",
    "these potential barriers'\n",
    "\n",
    "\n",
    "Questn_4='Do you think there are any unintended consequences arising from the \\\n",
    "children abolition of defence of reasonable punishment wales bill'\n",
    "\n",
    "\n",
    "Questn_5='Do you have any comments on the financial implications of the \\\n",
    "children abolition of defence of reasonable punishment wales bill'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Questn_6='Do you have any other points you wish to raise about the \\\n",
    "children abolition of defence of reasonable punishment wales bill'\n",
    "\n",
    "\n",
    "\n",
    "questn_asked=[Questn_0,Questn_1,Questn_2,Questn_3,Questn_4,Questn_5,Questn_6]\n",
    "\n",
    "\n",
    "\n",
    "questn_asked_short=['reasons for your support ?','need for legislation ?','potential barriers ?',\\\n",
    "                    'takes account of barriers ?','any unintended consequences ?',\\\n",
    "                   'financial implications ?','any other points ?']\n",
    "\n",
    "\n",
    "list_questns=['Questn_'+str(x) for x in range(7)]\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word,wn.NOUN)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "\n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word,'v')\n",
    "\n",
    "import string\n",
    "\n",
    "def clean_doc(text_record):\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    text_record_with_sentences=' '.join([get_lemma2(x.lower()) for\\\n",
    "                                         x in nltk.sent_tokenize(text_record)])\n",
    "    \n",
    "    text_record_with_sentences=re.sub('[^a-zA-Z0-9.]+',' ',text_record_with_sentences)\n",
    "    \n",
    "    #remove punct. except itra word hyphen\n",
    "    p = re.compile(r\"(\\b[-']\\b)|[\\W_]\")\n",
    "    text_record= p.sub(lambda m: (m.group(1) if m.group(1) else \" \"),text_record)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    tokens = text_record.split()\n",
    "    \n",
    "    # convert letters to lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    tokens_no_stpwrds = [get_lemma2(token) for token in tokens]\n",
    "    \n",
    "    \n",
    "    \n",
    "    tokens = [token for token in tokens if len(token) > 3]\n",
    "    \n",
    "    \n",
    "    \n",
    "    tokens= [get_lemma2(token) for token in tokens]\n",
    "    \n",
    "    tokens_copy = [token for token in tokens]\n",
    "    \n",
    "    \n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    sentence = ' '.join(tokens)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sentence_copy = ' '.join(tokens_copy)\n",
    "    \n",
    "    sentence_no_stop_words=' '.join(tokens_no_stpwrds)\n",
    "    \n",
    "    \n",
    "    return sentence,sentence_no_stop_words,text_record_with_sentences,sentence_copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the simplest, lambda-based implementation\n",
    "def multiple_replace(adict, text):\n",
    "  # Create a regular expression from all of the dictionary keys\n",
    "  regex = re.compile(\"|\".join(map(re.escape, adict.keys(  ))))\n",
    "\n",
    "  # For each match, look up the corresponding value in the dictionary\n",
    "  return regex.sub(lambda match: adict[match.group(0)], text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "str_responses=[[]]*numb_support_bill\n",
    "\n",
    "str_responses_copy=[[]]*numb_support_bill\n",
    "\n",
    "str_responses_no_stpwrds=[[]]*numb_support_bill\n",
    "str_responses_no_stpwrds_sentences=[[]]*numb_support_bill\n",
    "\n",
    "#Text-cleaning\n",
    "for index in range(numb_support_bill):\n",
    "    temp=[]\n",
    "    temp_copy=[]\n",
    "    temp_no_stpwrds=[]\n",
    "    temp_no_stpwrds_sentences=[]\n",
    "    for secnd_index in range(len(all_responses)):\n",
    "        data=all_responses[secnd_index][index].split('/~~/')\n",
    "        data=[x for x in data if (x!='-') & (x!='nan')&(len(x)!=0)]\n",
    "        \n",
    "        data=[x.replace('\\x92',\"\\'\") for x in data]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Remove Emails\n",
    "        data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "        \n",
    "        #Remove websites\n",
    "        \n",
    "        #data=[re.sub(r'^https?:\\/\\/.*[\\r\\n]*','',sent) for sent in data]\n",
    "        \n",
    "        data=[re.sub(r'http\\S+','',sent) for sent in data]\n",
    "\n",
    "        # Remove new line characters\n",
    "        data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "        # Remove distracting single quotes\n",
    "        #data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "        \n",
    "        \n",
    "        data=[expand_contractions(sent) for sent in data]\n",
    "        \n",
    "        \n",
    "        \n",
    "        data_unclean=[nltk.sent_tokenize(x) for x in data]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        data = [[clean_doc(comment)[0] for comment in y] for y in data_unclean]\n",
    "        \n",
    "        \n",
    "        data_copy = [[clean_doc(comment)[3] for comment in y] for y in data_unclean]\n",
    "        \n",
    "        \n",
    "        data_no_stpwrds = [[clean_doc(comment)[1] for comment in y] for y in data_unclean]\n",
    "        \n",
    "        \n",
    "        data_no_stpwrds_sentences = [[clean_doc(comment)[2] for comment in y] for y in data_unclean]\n",
    "        \n",
    "        \n",
    "        \n",
    "        data = [[x for x in y if len(x)!=0] for y in data]\n",
    "        \n",
    "        data_copy = [[x for x in y if len(x)!=0] for y in data_copy]\n",
    "        \n",
    "        data_no_stpwrds = [[x for x in y if len(x)!=0] for y in data_no_stpwrds]\n",
    "        \n",
    "        data_no_stpwrds = [[x for x in y if len(x)!=0] for y in data_no_stpwrds_sentences]\n",
    "        \n",
    "        \n",
    "        data=[[multiple_replace(dict_abvn,x) for x  in y] for y in data]\n",
    "        \n",
    "        data_copy=[[multiple_replace(dict_abvn,x) for x  in y] for y in data_copy]\n",
    "        data_no_stpwrds=[[multiple_replace(dict_abvn,x) for x  in y] for y in data_no_stpwrds]\n",
    "        data_no_stpwrds_sentences=[[multiple_replace(dict_abvn,x) for x  in y] for y in data_no_stpwrds_sentences]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        temp.append(data)\n",
    "        temp_copy.append(data_copy)\n",
    "        temp_no_stpwrds.append(data_no_stpwrds)\n",
    "        temp_no_stpwrds_sentences.append(data_no_stpwrds_sentences)\n",
    "        \n",
    "    \n",
    "    str_responses[index]=temp\n",
    "    str_responses_copy[index]=temp_copy\n",
    "    str_responses_no_stpwrds[index]=temp_no_stpwrds\n",
    "    str_responses_no_stpwrds_sentences[index]=temp_no_stpwrds_sentences\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "combined_responses_all_questns=[]\n",
    "combined_responses_all_questns_copy=[]\n",
    "combined_responses_all_questns_no_stpwrds=[]\n",
    "combined_responses_full_sentences=[]\n",
    "for x in range(len(str_responses)):\n",
    "    for y in range(len(questn_asked)):\n",
    "        \n",
    "        temp=[item for sublist in str_responses[x][y] for item in sublist]\n",
    "        \n",
    "        temp_copy=[item for sublist in str_responses_copy[x][y] for item in sublist]\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        temp_no_stpwrds=[item for sublist in str_responses_no_stpwrds[x][y] for item in sublist]\n",
    "        \n",
    "        \n",
    "        temp_full_statment=[' '.join(x).strip() for x in str_responses_no_stpwrds_sentences[x][y]]\n",
    "        \n",
    "        \n",
    "        combined_responses_all_questns.append(temp)\n",
    "        \n",
    "        combined_responses_all_questns_copy.append(temp_copy)\n",
    "        \n",
    "        combined_responses_all_questns_no_stpwrds.append(temp_no_stpwrds)\n",
    "        \n",
    "        combined_responses_full_sentences.append(temp_full_statment)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def gen_bi_trigram_key_wrds(rspns_list,minm_count,thrsh):\n",
    "    \n",
    "    docmnt_list=rspns_list\n",
    "    mincnt=minm_count\n",
    "    sentence_stream = [doc.split(\" \") for doc in docmnt_list]\n",
    "    bigram = Phrases(sentence_stream, min_count=mincnt,delimiter=b' ',threshold=thrsh)\n",
    "    trigram = gensim.models.Phrases(bigram[sentence_stream],min_count=mincnt,delimiter=b' ',threshold=thrsh) \n",
    "\n",
    "\n",
    "    bigrams_ = [[b for b in bigram[sent] if b.count(' ') == 1] for sent in sentence_stream]\n",
    "    bigrams_ = [item for sublist in bigrams_ for item in sublist]\n",
    "    trigrams_ = [[t for t in trigram[bigram[sent]] if t.count(' ') == 2] for sent in sentence_stream]\n",
    "    trigrams_ = [item for sublist in trigrams_ for item in sublist]\n",
    "    bigrams_=[b for b in bigrams_ if all(b not in t for t in trigrams_)]\n",
    "    combined_list=bigrams_+trigrams_\n",
    "    flat_list=combined_list\n",
    "    return flat_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.6: Finding most frequent word collocation phrases  in responses in support or otherwise for the Bill\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occur=10\n",
    "thrshld=5\n",
    "str_responses_key_wrds=[0]*len(typ_respons)\n",
    "for index in range(len(typ_respons)):\n",
    "                                             \n",
    "    str_temp=[]\n",
    "    for qustn_indx in range(len(questn_asked)):\n",
    "        data=combined_responses_all_questns[len(questn_asked)*index:len(questn_asked)*(index+1)][qustn_indx]\n",
    "        temp=gen_bi_trigram_key_wrds(data,min_occur,thrshld)\n",
    "        str_temp.append(temp)\n",
    "    str_temp=[item for sublist in str_temp for item in sublist]\n",
    "          \n",
    "    str_responses_key_wrds[index]=str_temp\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "flat_list_all_keywrds=str_responses_key_wrds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# we can override the default parameters\n",
    "fnt_sz=18\n",
    "\n",
    "\n",
    "fnt_sz_big=25\n",
    "\n",
    "fnt_sz_tk=15\n",
    "\n",
    "fnt_sz_tk_big=12\n",
    "\n",
    "fnt_sz_legend_big=12 \n",
    "fig_siz=(10,5)\n",
    "fig_siz_big=(35,18)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rotn_tld=15\n",
    "\n",
    "\n",
    "#Getting the most frequent words in each response type\n",
    "\n",
    "top_words=100\n",
    "\n",
    "top_few_words=20\n",
    "\n",
    "WC_height = 1000\n",
    "WC_width = 2000\n",
    "WC_max_words = 100\n",
    "\n",
    "\n",
    "\n",
    "for index in range(len(typ_respons)):\n",
    "    \n",
    "    freq_flat_list=nltk.FreqDist(flat_list_all_keywrds[index])\n",
    "    \n",
    "    word_dict={}\n",
    "    ordered_list=sorted(freq_flat_list.items(),key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    freq_dis_plot = plt.figure()\n",
    "\n",
    "    plt.figure(figsize=fig_siz_big)\n",
    "    df=pd.DataFrame(freq_flat_list.most_common(top_words))\n",
    "    \n",
    "    \n",
    "    if df.shape[0]!=0:\n",
    "        df.columns=['Key phrases', 'Counts']\n",
    "        df.to_csv(path_to_results+'most_frequent_'+str(typ_respons[index])+'.csv')\n",
    "        df=df.head(top_few_words)\n",
    "        df.plot(x='Key phrases',y='Counts',kind='bar',figsize=fig_siz_big)\n",
    "        plt.xlabel('Most frequent phrases', fontsize = fnt_sz)\n",
    "        plt.ylabel('Frequency count', fontsize = fnt_sz_big)\n",
    "        plt.legend(['Top {} common phrases when respondents replied {} in support of the bill'\\\n",
    "                .format(str(top_few_words),str(typ_respons[index]).lower())],fontsize = fnt_sz_big)\n",
    "        plt.xticks(fontsize=fnt_sz,rotation=rotn_tld)\n",
    "        plt.yticks(fontsize=fnt_sz,rotation=rotn_tld)\n",
    "        plt.savefig(path_to_results+'most_frequent_count_'+'_'+str(typ_respons[index]).lower()+'.png')\n",
    "        \n",
    "        \n",
    "        plt.show()\n",
    "        wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width)\n",
    "        wordCloud.generate_from_frequencies(dict(ordered_list))\n",
    "        plt.title(['Most common phrases when respondents respondents replied {} in support of the bill'\\\n",
    "                .format(str(typ_respons[index]).lower())],fontsize = fnt_sz)\n",
    "        plt.imshow(wordCloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        wordCloud.to_file(path_to_results+'word_cloud_most_frequent_count_'+'_'+str(typ_respons[index]).lower()+'.png')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.7: Finding context around the most frequent word collocation phrases in responses in support or otherwise for the Bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "for which in typ_respons:\n",
    "    top_n=top_few_words\n",
    "    index_chsen=[x for x,y in enumerate(typ_respons) if y==which][0]\n",
    "\n",
    "\n",
    "\n",
    "    if os.path.isfile(path_to_results+'most_frequent_'+str(which)+'.csv'):\n",
    "        df_key_wrd=pd.read_csv(path_to_results+'most_frequent_'+str(which)+'.csv')\n",
    "        \n",
    "        df_key_wrd=df_key_wrd.head(top_n)\n",
    "    \n",
    "        lst_ky_wrds=list(df_key_wrd['Key phrases'])\n",
    "\n",
    "        print(index_chsen)\n",
    "\n",
    "\n",
    "\n",
    "        str_context_all=[]\n",
    "        str_df_key_top_n=pd.DataFrame()\n",
    "        for out_indx in range(len(lst_ky_wrds)):\n",
    "            str_tmp=[]\n",
    "            for indx in range(len(questn_asked)):\n",
    "                flat_respns=[item for sublist in str_responses_no_stpwrds[index_chsen][indx] for item in sublist if item!=[]]\n",
    "    \n",
    "        \n",
    "                str_context=[item for item  in flat_respns if re.search(lst_ky_wrds[out_indx],' '.join([ get_lemma2(x) for x in item.split()\\\n",
    "                                                                                                if x not in stop_words]))]\n",
    "        \n",
    "                str_context=[item for item  in str_context if item!=[]]\n",
    "                tmp=str_context\n",
    "                str_tmp.append(tmp)\n",
    "            str_tmp=[item for sublist in str_tmp for item in sublist if item!=[]]\n",
    "            str_context_all.append(str_tmp)\n",
    "        \n",
    "        pd.set_option('display.max_colwidth',-1)\n",
    "        str_df_key_top_n['Frequent Phrases']=['----'.join(x) for x in str_context_all]\n",
    "\n",
    "        str_df_key_top_n['Keywords']=lst_ky_wrds\n",
    "\n",
    "        str_df_key_top_n['Support to the bill']=[str(which)]*len(lst_ky_wrds)\n",
    "\n",
    "        str_df_key_top_n.to_html(path_to_results+'key_phrases_'+str(which)+'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.8: Finding salient phrases (using tf-idf)  in all the responses in support or otherwise for the Bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for tf-idf\n",
    "max_cnt=0.1# (ignore phrases present in more than 10% of the responses)\n",
    "min_cnt=5 # (ignore phrases present in less than 5 of the responses)\n",
    "\n",
    "top_n=20\n",
    "\n",
    "\n",
    "min_cnt_mean=0.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes a single row of the tf-idf matrix \n",
    "#(corresponding to a particular document), \n",
    "# and return the n highest scoring words \n",
    "#(or more generally tokens or features):\n",
    "#top_tfidf_feats(train_unigrams.toarray()[0,:],\n",
    "#features, top_n)\n",
    "def top_tfidf_feats(row, features, top_n):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#The result of a tf-idf, however, is typically a sparse matrix, \n",
    "#which doesn’t support all the usual matrix or array operations. \n",
    "#So in order to apply the above function to inspect a particular document,\n",
    "#we convert a single row into dense format first:\n",
    "def top_feats_in_doc(Xtr, features, row_id, top_n):\n",
    "    'Top tfidf features in specific document (matrix row)'\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)\n",
    "\n",
    "\n",
    "\n",
    "# gives us the most important words across the whole corpus.\n",
    "#Here are the top_n: Here, we provide a list of row indices which \n",
    "#pick out the particular documents we want to inspect. \n",
    "#Providing ‘None’ indicates, somewhat counterintuitively, that we’re \n",
    "#interested in all documents. We then calculate the mean of each column across the selected rows,\n",
    "#which results in a single row of tf-idf values. And this row we then simply pass on to our previous\n",
    "#function for picking out the top n words. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids, min_tfidf=min_cnt_mean, top_n=top_n):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids,:].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "\n",
    "#What might be more interesting, though, is to separately consider\n",
    "#groups of documents falling into a particular category. For example,\n",
    "#let’s calculate the mean tf-idf scores\n",
    "#depending on a document’s class label:\n",
    "\n",
    "# modified for multilabel milticlass\n",
    "def top_feats_by_class(Xtr, features, min_tfidf=min_cnt_mean, top_n=top_n):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    cols=subtypes\n",
    "    for col in cols:\n",
    "        ids = train_tags.index[train_tags[col]==1]\n",
    "        feats_df = top_mean_feats(Xtr, features, list(ids), min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotn_tld_2=12\n",
    "\n",
    "for index in range(len(typ_respons)):\n",
    "    \n",
    "    #respons=combined_responses_all_questns[index]\n",
    "    \n",
    "    \n",
    "    respons=[item for sublist in combined_responses_all_questns[len(questn_asked)*index:len(questn_asked)*(index+1)] for \\\n",
    "            item in sublist]\n",
    "    clean_corpus=pd.DataFrame(respons)\n",
    "    \n",
    "    \n",
    "    \n",
    "    tfv = TfidfVectorizer(max_df=max_cnt,min_df=min_cnt,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',ngram_range=(2,3),\n",
    "            use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = stop_words)\n",
    "\n",
    "    tfv.fit(clean_corpus[clean_corpus.columns[0]])\n",
    "    features = np.array(tfv.get_feature_names())\n",
    "    train_ngrams =  tfv.transform(clean_corpus[clean_corpus.columns[0]].ravel())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    most_imp_wrds_df=top_mean_feats(train_ngrams, features, None, min_tfidf=min_cnt_mean, top_n=2*top_n)\n",
    "    \n",
    "    most_imp_wrds_df=most_imp_wrds_df[most_imp_wrds_df['tfidf']!=0]\n",
    "\n",
    "    most_imp_wrds_df_copy=most_imp_wrds_df.head(top_n)\n",
    "\n",
    "    most_imp_wrds_df_copy.plot(x='feature',y='tfidf',kind='bar',figsize=fig_siz_big)\n",
    "    \n",
    "    \n",
    "     \n",
    "\n",
    "    plt.xlabel('Phrases', fontsize = fnt_sz_big)\n",
    "    plt.ylabel('Importance (tfidf score)', fontsize = fnt_sz_big)\n",
    "    plt.legend(['{} top most important phrases\\\n",
    "    when respondent replied {} in support of the bill'.\\\n",
    "                format(most_imp_wrds_df_copy.shape[0],str(typ_respons[index]).lower())],fontsize = fnt_sz_big)\n",
    "    plt.xticks(fontsize=fnt_sz,rotation=rotn_tld_2)\n",
    "    plt.yticks(fontsize=fnt_sz)\n",
    "    \n",
    "    plt.savefig(path_to_results+'top {} most_important_words when respondent replied {}\\\n",
    "    in support of the bill'.format(most_imp_wrds_df.shape[0],str(typ_respons[index]).lower()))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "                \n",
    "    from wordcloud import WordCloud\n",
    "    old_dic=dict(zip(most_imp_wrds_df.feature,list(most_imp_wrds_df.tfidf)))\n",
    "    new_dic = {k:v for k,v in old_dic.items() if v != 0}\n",
    "    \n",
    "    wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width)\n",
    "    wordCloud.generate_from_frequencies(new_dic)\n",
    "    plt.title(path_to_results+'most_important_words when respondent replied {}\\\n",
    "    '+'_'+str(typ_respons[index]).lower())\n",
    "    plt.imshow(wordCloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    #wordCloud.to_file(path_to_results+'word_cloud_most_imp_count_'+'_'+str(typ_respons[index]).lower()+'.png')\n",
    "    \n",
    "    wordCloud.to_file(path_to_results+'word_cloud_most_important_words_'+'_'+str(typ_respons[index]).lower()+'.png')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
