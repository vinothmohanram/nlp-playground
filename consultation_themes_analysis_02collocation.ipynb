{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed015bfe-bbdc-498b-ae3e-a47ccad17155",
   "metadata": {},
   "source": [
    "# Themes Analysis for Consultation Sandbox\n",
    "## Phrase Collocation\n",
    "\n",
    "This notebook is a test of extraction of key themes from dummy consultation data.\n",
    "Inspired by: https://datasciencecampus.ons.gov.uk/projects/automating-consultation-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c22719-d423-479b-8f13-fb80672bc8fd",
   "metadata": {},
   "source": [
    "---\n",
    "## Technique B: Collocation\n",
    "\n",
    "Method used taken from: https://medium.com/@nicharuch/collocations-identifying-phrases-that-act-like-individual-words-in-nlp-f58a93a2f84a#:~:text=The%20two%20most%20common%20types,or%20'Proctor%20and%20Gamble'.![image.png](attachment:c7f00693-1e26-471d-b947-308712fda132.png)![image.png](attachment:45859265-852c-4255-b361-9df0682b3558.png)![image.png](attachment:3c627536-5b49-4758-932b-ea94afc021e5.png)![image.png](attachment:95cc1a5e-a5e0-401c-9b3a-786f91a83445.png)\n",
    "\n",
    "We'll start by counting the frequency of bigrams and trigrams\n",
    "\n",
    "---\n",
    "\n",
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee830cb-f5e3-4964-90b2-5243b51bbcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages \n",
    "\n",
    "from arrow_pd_parser import reader\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803dadfb-f7f3-4c7c-a330-686242f83dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "s3_bucket = \"s3://alpha-everyone/nlp-code-examples/\"\n",
    "file_loc = \"Consultation_Dummy_NewQuestions.csv\"\n",
    "\n",
    "df = reader.read(os.path.join(s3_bucket, file_loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b745623a-7fbe-4125-9b8c-88c73e840d18",
   "metadata": {},
   "source": [
    "Clean column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce79d0-78b7-46dd-8987-d4420051c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_replace(replacements, text):\n",
    "    # Create a regular expression from the dictionary keys\n",
    "    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, replacements.keys())))\n",
    "    # For each match, look-up corresponding value in dictionary\n",
    "    return regex.sub(lambda mo: replacements[mo.group()], text) \n",
    "\n",
    "def multiple_replace(replacements, text):\n",
    "    # Create a regular expression from the dictionary keys\n",
    "    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, replacements.keys())))\n",
    "    # For each match, look-up corresponding value in dictionary\n",
    "    return regex.sub(lambda mo: replacements[mo.group()], text) \n",
    "\n",
    "replacements = {\" \":\"_\",\n",
    "              \"-\":\"_\",\n",
    "              \"/\":\"_\",\n",
    "              \"?\":\"\",\n",
    "              \"'\":\"\"}\n",
    "\n",
    "new_cols = list()\n",
    "for i in df.columns.str.split('- '):\n",
    "    cleaned = multiple_replace(replacements, i[-1]).lower().strip()\n",
    "    new_cols.append(cleaned)\n",
    "df.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f66ab-d78a-4f48-ab13-d6e3534d3eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b8bc42-4e79-4d7a-81e2-473f2f83cd62",
   "metadata": {},
   "source": [
    "Define data cleansing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41637587-117c-4223-afba-320d8fa7a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean and lemmatize comments\n",
    "def clean_comments(text):\n",
    "    #remove punctuations\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", str(text))\n",
    "    #use spacy to lemmatize comments\n",
    "    doc = nlp(nopunct, disable=['parser','ner'])\n",
    "    lemma = [token.lemma_ for token in doc]\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a3b587-a603-47ca-81c9-e07910940b21",
   "metadata": {},
   "source": [
    "Cleanse data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b4bf8-5920-402f-a781-f4a65614c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_col = \"what_are_the_positives_of_the_pilot_scheme\"\n",
    "comments = df[comments_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc8c34-d993-4667-a9fc-54e7c891207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function to clean and lemmatize comments\n",
    "df[\"comments_lemm\"] = df[comments_col].map(clean_comments)\n",
    "\n",
    "#make sure to lowercase everything\n",
    "df[\"comments_lemm\"] = df[\"comments_lemm\"].map(lambda x: [word.lower() for word in x])\n",
    "\n",
    "#turn all comments' tokens into one single list\n",
    "unlist_comments = [item for items in df.comments_lemm for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b8200-92cc-4530-a5fc-d4f0321e66fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = unlist_comments\n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a988120-b2df-4de2-9631-d954b232ee06",
   "metadata": {},
   "source": [
    "Create bigrams and trigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1ed59-c286-49bb-a6d5-929a1e9c7e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c43910-e9a3-4ec4-8b63-f6c4cd9217d7",
   "metadata": {},
   "source": [
    "----\n",
    "#### a. Counting frequencies of adjacent words with part of speech filters:\n",
    "\n",
    "The simplest method is to rank the most frequent bigrams or trigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d1804-4ee7-4fee-9730-0009fd27e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams\n",
    "bigram_freq = bigramFinder.ngram_fd.items()\n",
    "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "#trigrams\n",
    "trigram_freq = trigramFinder.ngram_fd.items()\n",
    "trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e04ba-bd52-49f9-953e-df9680facbd4",
   "metadata": {},
   "source": [
    "However, a common issue with this is adjacent spaces, stop words, articles, prepositions or pronouns are common and are not meaningful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f0897-e852-4c09-937a-797be16c60e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramFreqTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9003f861-bc4f-40cd-a724-2676afdec6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramFreqTable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4125f470-15a8-4593-8839-dfaa12688def",
   "metadata": {},
   "source": [
    "To fix this, we filter out for collocations not containing stop words and filter for only the following structures:\n",
    "\n",
    "Bigrams: (Noun, Noun), (Adjective, Noun)\n",
    "\n",
    "Trigrams: (Adjective/Noun, Anything, Adjective/Noun)\n",
    "\n",
    "This is a common structure used in literature and generally works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2630c86a-14e5-477f-aa02-7e6baa946f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#get english stopwords\n",
    "en_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef210c-d4ab-4401-b752-4ee99951bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to filter for ADJ/NN bigrams\n",
    "def rightTypes(ngram):\n",
    "    if '-pron-' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords or word.isspace():\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0225500-4ef3-43be-a97a-fd016460c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter bigrams\n",
    "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n",
    "filtered_bi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c803ce7c-1d19-49fe-a67c-3088cbe6b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to filter for trigrams\n",
    "def rightTypesTri(ngram):\n",
    "    if '-pron-' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords or word.isspace():\n",
    "            return False\n",
    "    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in first_type and tags[2][1] in third_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea1656-5d64-40b8-9b5d-c5e9774382a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filter trigrams\n",
    "filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]\n",
    "filtered_tri.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314b39e-c1c0-40bd-86a6-2a8085bafad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tri.columns = [w.replace('trigram', 'gram') for w in filtered_tri.columns]\n",
    "filtered_bi.columns = [w.replace('bigram', 'gram') for w in filtered_bi.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11d49c-0b0f-46d7-89d1-bca52c6cc040",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_gram = pd.concat([filtered_tri, filtered_bi]).sort_values(\"freq\", ascending = False)\n",
    "filtered_gram.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a480f72-ad8d-4d30-9ac0-916491e64063",
   "metadata": {},
   "source": [
    "#### Create word cloud for key phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4804734-26a2-459c-a43e-904d11559f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c13ea6-ccad-45bc-bbe8-88c9e8230c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a dictionary from DataFrame\n",
    "words = filtered_gram.gram.apply(lambda x: \" \".join(x))\n",
    "freq = filtered_gram.freq\n",
    "word_freq_dict = dict(zip(words, freq))\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq_dict)\n",
    "\n",
    "# Plot the WordCloud image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off the axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74885e-8620-49be-b664-4a02b5c687c5",
   "metadata": {},
   "source": [
    "#### Next step: Extract context around key phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9be6e-45f1-4a54-a8b8-e154d373e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_gram.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7c089-534a-4bb1-a22a-60c64a955556",
   "metadata": {},
   "source": [
    "We'll extract the context for just the trigrams or bigrams that appear more than 5 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb68b31-aa6b-4220-8218-f40d0399a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_phrases = filtered_gram.loc[filtered_gram.freq > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df076ac5-40fe-4eeb-b97b-21284b34e255",
   "metadata": {},
   "source": [
    "We need to convert the lemmatized version of the text to a string (currently a list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10526e1-661b-4bb1-882c-eb3322df9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(list):\n",
    "    filtered_list = [element for element in list if element.strip()]\n",
    "    list = \" \".join(filtered_list)\n",
    "    return list\n",
    "\n",
    "df[\"comments_lemm_clean\"] = df.comments_lemm.map(list_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e8ef05-7fe1-440b-9e53-1e8934f76327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_surrounding_characters(full_string, target_string, chars = 50):\n",
    "    # Find the index of the target string in the full string\n",
    "    index = full_string.find(target_string)\n",
    "\n",
    "    # Check if the target string is present in the full string\n",
    "    if index != -1:\n",
    "        # Extract the surrounding characters\n",
    "        start_index = max(0, index - chars)\n",
    "        end_index = min(len(full_string), index + len(target_string) + chars)\n",
    "        surrounding_chars = full_string[start_index:end_index]\n",
    "\n",
    "        return surrounding_chars\n",
    "\n",
    "    return None    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065e2af-0580-4647-b468-210517860e61",
   "metadata": {},
   "source": [
    "We'll loop through all key phrases and create a column for each which is populated for each row where key phrase is present, with the context around each phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3187e3-01c8-47f5-aa87-5be73ffc6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all key phrases and create a column for each, \n",
    "# which is populated for each row where key phrase is present\n",
    "# with the context around the phrase\n",
    "for i in key_phrases.gram:\n",
    "    phrase = (\" \".join(i))\n",
    "    phrase_col = phrase.replace(\" \", \"_\")\n",
    "    df[phrase_col] = df['comments_lemm_clean'].apply(lambda x: extract_surrounding_characters(x, phrase))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa190f-269e-4135-b5e5-bf3e498de7ba",
   "metadata": {},
   "source": [
    "We'll extract this information into a seperate format, to give a column with the key phrases, and a column with the context around it each time it appears. This will have no inidividual's information - designed just to give a summary of the kind of context the phrase appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191353d4-3b59-4b57-8ec2-b0f027a129ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only context columns\n",
    "key_phrases_cols = key_phrases.gram.apply(lambda x: \"_\".join(x)).tolist()\n",
    "df_key_phrases = df[key_phrases_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef3db2-5c23-41d0-b152-3bdc38252dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack columns using pd.melt()\n",
    "df_key_phrases = pd.melt(df_key_phrases, var_name = \"key_phrase\", value_name = \"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4571a-d35b-4f0d-a69f-1b7a7ce2b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out 'none' values\n",
    "df_key_phrases = df_key_phrases[~df_key_phrases.context.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0896eda4-d367-4354-a656-def63e74e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_phrases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20015b5e-a8f9-45e7-914d-29dda7dcc28e",
   "metadata": {},
   "source": [
    "----\n",
    "#### b. Alternative approach: Pointwise Mutual Information\n",
    "\n",
    "The main intuition is that it measures how much more likely the words co-occur than if they were independent. However, it is very sensitive to rare combination of words. For example, if a random bigram ‘abc xyz’ appears, and neither ‘abc’ nor ‘xyz’ appeared anywhere else in the text, ‘abc xyz’ will be identified as highly significant bigram when it could just be a random misspelling or a phrase too rare to generalize as a bigram. Therefore, this method is often used with a frequency filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7866d-4ddc-4cd9-b626-03213c308623",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "trigrams = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6c996-4115-4111-b74d-06c5a0a672c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only those with more than 20 occurences\n",
    "bigramFinder.apply_freq_filter(20)\n",
    "trigramFinder.apply_freq_filter(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad02e28-c87d-4dd2-9e73-f6307ac80e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramPMITable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.pmi)), columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)\n",
    "bigramPMITable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39256a1d-7fb7-49a7-8964-b2127bc8dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramPMITable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.pmi)), columns=['trigram','PMI']).sort_values(by='PMI', ascending=False)\n",
    "trigramPMITable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81ff96-c4a5-4865-89e5-e4eeeb1521c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_tri = trigramPMITable.copy()\n",
    "pmi_bi = bigramPMITable.copy()\n",
    "\n",
    "pmi_tri.columns = [w.replace('trigram', 'gram') for w in trigramPMITable.columns]\n",
    "pmi_bi.columns = [w.replace('bigram', 'gram') for w in bigramPMITable.columns]\n",
    "\n",
    "pmi_gram = pd.concat([pmi_tri, pmi_bi]).sort_values(\"PMI\", ascending = False)\n",
    "pmi_gram.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1459e4-dc42-4e19-b08f-aa3d88668f43",
   "metadata": {},
   "source": [
    "#### Create word cloud for key phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89642b12-dff4-452d-a690-61600af7288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b6ce9-c027-460a-882c-d3a157a8199d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a dictionary from DataFrame\n",
    "words_pmi = pmi_gram.gram.apply(lambda x: \" \".join(x))\n",
    "pmi = pmi_gram.PMI\n",
    "word_pmi_dict = dict(zip(words_pmi, pmi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d7628-b6ef-457a-9e18-49187a0325bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the word cloud\n",
    "wordcloud2 = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_pmi_dict)\n",
    "\n",
    "# Plot the WordCloud image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off the axis labels\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp_code_examples_20240130",
   "language": "python",
   "name": "venv_nlp_code_examples_20240130"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
